---
layout: post
title: "Testing robotics systems in fast-paced startups"
date: 2020-12-16 00:00:00 -0800
updated: 2022-06-29 00:00:00 -0800
tags: ["#software-engineering"]
comments: true
---

<figure>
  <img src="https://live.staticflickr.com/195/506281600_a68f821d33_c.jpg" width="480px">
  <figcaption>Starcraft II, Photo by <a href="https://www.flickr.com/photos/tirrell/">Zach Tirrell</a> on <a href="https://www.flickr.com/">flickr</a></figcaption>
</figure>

Testing robotics systems is hard.
Based on my limited experience working at startups with fewer than 200 employees and fewer than 100 robots providing RaaS using fleets of indoor mobile robots or lines of robot manipulators, the main reasons for the difficulty were as follows:

1. _Edge cases and corner cases in production environments._
1. _The difficulty of using simulation._
1. _Challenges with adopting automation._


## Why is testing robotics systems hard?

In **production**, I've encountered various **edge cases and corner cases**:

1. _Edge cases for robotics algorithms._
    Input spaces for robotics algorithms, such as perception, control, and motion planning, are vast and challenging to effectively cover for edge cases; there is always a specific layout that causes navigation failures or a particular scene with specific objects that leads to grasping failures.
    Characterizing such instances is difficult and algorithm-dependent, which complicates the testing setup.
1. _Rare hardware issues._
    Rare hardware issues that are not (directly) detectable are the worst, such as a small damage in the robot cell structure that requires adjusting the collision map.
    Anticipating such issues requires input from domain experts (e.g., mechanical or firmware engineers), who may not be easily accessible and speak different jargons and reproducing them often requires changing interfaces, which can be expensive (e.g., it becomes yet another layer to maintain).
1. _Subtle regression._
    The complexity of robotics systems makes it challenging to establish a robust [regression testing](https://katalon.com/resources-center/blog/regression-testing) pipeline.
    For example, handling low-frequency [flaky tests](https://docs.gitlab.com/ee/development/testing_guide/flaky_tests.html), implementing robust [test selection and prioritization](https://damorimrg.github.io/practical_testing_book/testregression/selectionprio.html)[^2] is difficult and hence elusive bugs slip back into the production code.
    Performance regressions are particularly challenging--especially ones caused by low-level concurrency issues[^3], as they are subtle and require expensive measures such as repeated end-to-end tests and delicate statistical methods to detect.
1. _Unexpected peak load condition/usage pattern._
    It is common for multiple (custom) software, such as core robotics, monitoring, and infra-related software, to run in parallel.
    Unexpected high demands can adversarially impact your program, e.g., by consuming all of the available resources.
    Anticipating and recreating such situations is challenging, especially when dealing with (custom) software at all levels, including firmware and system software.

**Using simulation** for testing robotics systems effectively **is not as easy** as it seems.

1. _Inadequate usage._
    I find that simulation testing is most useful for end-to-end testing of robot applications.
    However, I often encounter test cases that would benefit from using other tools and techniques (e.g., for efficiency).
    All too frequently, I come across test cases for robot behaviors (e.g., implemented in finite state machine or behavior tree) that use simulation, making the test cases much more expensive than they need to be.
    In such cases, using alternatives like [fake](https://martinfowler.com/bliki/TestDouble.html) or [model-based testing](https://www.educative.io/answers/what-is-model-based-testing)[^4] would be much more efficient, as they can be used to only "simulate" the directly relevant modules[^5].
    This often stems from organizational issues, such as unclear boundaries between teams that result in poorly defined interfaces and testing strategies[^6] or more typical insufficient allocation of time for testing/addressing technical debts (e.g., in favor of prioritizing other deliverables).
1. _Generating test cases._
    Even with simulation libraries that provide high-level interfaces for building scenarios, creating effective test scenarios is challenging.
    Creating a single simulated environment for end-to-end testing alone is laborious enough, so diversifying the test scenarios (e.g., to cover extreme cases) becomes a nice-to-have[^7].
    There are commercial products that address this issue (e.g., [AWS RoboMaker WorldForge](https://aws.amazon.com/blogs/aws/aws-announces-worldforge-in-aws-robomaker/)), but they are not easy for smaller organizations (i.e., startups) to integrate due to reasons such as integration cost, vendor lock-in, etc.
1. _Expressing specifications._
    Specifications for many robotics programs, e.g., those involving perception, motion planning, and behaviors, are difficult to express due to their spatiotemporal nature.
    This leads to verbose and unorganized (e.g., containing duplicates) test code, which makes it difficult to maintain and scale.
1. _Managing infrastructure._
    I haven't met a single person who loves managing simulation testing infrastructure, e.g., for continuous integration.
    Simulation test code is expensive to run, requires special hardware such as GPUs, and is difficult to optimize and move around (e.g., in cloud environments).
    This leads to a poor developer experience and can even result in the disabling of simulation testing.

**Automating** robotics software **testing is still hard**.

1. _Challenges with automating build and deployment._
    Here are tech talks and a blog post that shed light on this topic:
    - ["Building Self Driving Cars with Bazel"](https://youtu.be/fjfFe98LTm8) from Cruise, BazelCon 2019 - shares Cruise's experiences with building and testing robotics software at scale
    - ["The landscape of software deployment in robotics"](https://www.airbotics.io/blog/software-deployment-landscape) from Airbotics - summarizes the typical challenges with deploying robotics software
    - ["Physical continuous integration on real robots"](https://youtu.be/JNV9CkARh_g) from Fetch, ROSCon 2016 - shares Fetch's experience with setting up and using a physical continuous integration pipeline
    - ROS (2) docs on Build (tools): [catkin/conceptual_overview](http://wiki.ros.org/catkin/conceptual_overview), [A universal build tool](https://design.ros2.org/articles/build_tool.html), [About the build system](https://docs.ros.org/en/iron/Concepts/About-Build-System.html)
    - ROS (2) docs on Deployment (tools): [Deployment Guidelines](https://docs.ros.org/en/iron/Tutorials/Advanced/Security/Deployment-Guidelines.html), [bloom](http://wiki.ros.org/bloom)
1. _No standard._
    Automating the testing of software requires agreements among engineering teams on build, deployment, and test models.
    Given how robotics brings multiple communities together, such as research (e.g., computer vision, robotics), web development (e.g., frontend, backend), DevOps, embedded, etc., reaching such an agreement, or even discussing ideas (e.g., due to different backgrounds), is difficult.
    While the Robot-Operating System (ROS) and the communities around it have made significant progress in this regard, the lack of standards still seems to be a significant problem in organizations.


## Where to start with testing: general techniques

To figure out where to start with testing a robotics system, I use (1) a prioritization framework for creating tests and (2) a systematic procedure for identifying what to test.
The techniques discussed in this section are general/not-so-technical and are mostly aimed at addressing the first type of challenges mentioned above, i.e., "edge and corner cases in a production environment."
<!-- TODO: write a sentence _for {something}, read the next section ..._-->

### Eisenhower matrix for test prioritization

In robotics startups that build complex systems, creating comprehensive test suites is impossible.
To produce high-impact tests within the time budget, I use my adapted [Eisenhower Matrix](https://www.eisenhower.me/eisenhower-matrix/) to prioritize a list of failure scenarios by first categorizing (potential) failure scenarios according to their (expected) frequency and risk.

<figure>
  <img src="/assets/imgs/mcmatrix.png" width="480px">
</figure>

1. _First Quadrant (upper left): frequent and high-risk._
    In Quadrant 1 (Q1), I place failure scenarios that need to be covered immediately, e.g., that I hear all the time from internal communication channels, such as introducing breaking changes to APIs and dependencies.
1. _Second Quadrant (upper right): frequent and medium-risk._
    In Quadrant 2 (Q2), I place failure scenarios that occur frequently but allow continued operations with short downtime like unreliable hardware or unresponsive user interface issues with well-established alerts and recovery procedures.
1. _Third Quadrant (lower left): infrequent and high-risk._
    In Quadrant 3 (Q3), I place failure scenarios that occur rarely but causes significant disruption in operations such as core robotics component failure scenario or unexpected peak usage pattern.
1. _Fourth Quadrant (lower right): infrequent and medium-risk._
    In Quadrant 4 (Q4), I place failure scenarios that occur relatively infrequently and allow continued operations.

The placement of example failure scenarios in quadrants will differ across companies.
For instance, depending on the maturity of the robot product/prototype or the amount of time invested by the engineering team in designing the system, an unreliable hardware failure scenario may belong in Q1 (e.g., if it is causing multiple issues) or a robotics algorithms failure scenario may belong in Q2 (e.g., if the failure is not catastrophic or easily recoverable).
In general, I create or improve tests for one quadrant at a time, in increasing order.
After working on tests for Q1, I move on to tests for Q2 before addressing those for Q3.
This is because creating tests for Q3 requires a significant time investment, for example, to ensure the reproducibility of the failure.
Usually, there is no time available to work on tests for Q4.
But adjustments should be made to meet organization-specific requirements and constraints.

### Test scenario identification procedure

So far, I have assumed that the failure scenarios to test are known; however, this is usually not the case.
To determine what to test, I follow these steps:

1. _Gain access to internal alerts, dashboards, and logs._
    Investigating recently reported problems or analyzing the latest trends using monitoring tools[^8] is the easiest way to identify high-risk failure scenarios.
    If monitoring tools are not set up (e.g., in smaller companies), I get involved in operations work, which is another way to uncover potential high-value tests to create.
1. _Identify interface and service boundaries._
    Understanding how software components interact with each other provides insights into potential integration failures and their impact.
    I start by looking for internal documentation with system diagrams (or examining the codebase and creating them if such diagrams don't exist) and ask questions such as: which interactions must not fail? which interactions are changing frequently?
    Such exercises reveal missing must-have contract tests or high-impact opportunities to improve integration tests.
1. _Identify implicit dependencies._
    I consider edge cases such as low resources, unexpected hardware states, or unseen inputs to robotics algorithms (e.g., those that crash applications) as unmet runtime dependencies.
    Taking this view nudges me to specify these not-well-understood requirements for keeping the system (or "implicit dependencies") well-behaving as explicitly and clearly as possible.
    Once defined, such requirements can be used to create extreme failure scenarios to test.

## Ensuring testability as a startup grows

> This section is work-in-progress.\
> Please check back for updates!

Below I share my insights on key practices to employ at each growth stage/funding round of robotics startups.
Since the real motivation behind testing is the reliabilty (e.g., of the provided service) and so the shared key practices below covers related areas such as debugging and observability[^9].

### Series A: 5-20 employees

At this stage, startups are likely to have fewer than 5 customers/design partners, and a handful of developers who are relentlessly building (and fixing) major components of the company's first product.
The goal of the startups is to prove the value of their product to their (rather forgiving) customers by succeeding in basic tasks performed by robots as much as possible.
For example, a delivery robot should navigate without colliding with obstacles, and a robot manipulator should pick and place objects without dropping them in customers' (production) environments.

**Key Practices:**

- Setting up _continuous integration or nightly tests_ (e.g., using GitHub Actions, GitLab CI/CD, Jenkins) with at least one _end-to-end test involving a high-fidelity simulator_ (e.g., Gazebo) to smoke test quickly changing codebases.
- Setting up _internal communication channels_ (e.g., Slack) and investing in a reliable _teleop solution_ (e.g., built on MQTT, WebRTC) for quickly responding to critical incidents.
- Creating _metrics and dashboards_ (e.g., using Graphana/Prometheus) _to track business-critical measures_ such as the number of deliveries/distance traveled, throughput/object knitted.

### Series B: 21-200 employees

Startups at this stage start expanding their customer base and (aim to) scale their operations to deploy and handle, for example, more than 100 robots.
The companies now have (small) teams of developers working on enhancing the robustness of core robotics software components to handle diverse environments of new customers, providing non-beta-user-acceptable user experience (e.g., by building a proper onboard and/or desktop UIs), and/or infrastructure to scale operations.
The robotics system powering the product becomes much more complex and to manage such a complexity, its architecture involves (e.g., becomes more modular, composable, and distributed), which establishes boundaries in results, and developer teams start relying on toolings.

**Key Practices:**

<!-- (e.g., using pytest, GoogleTest) -->
<!-- - Employing test-driven-development to test each compoennt in isolation  -->
<!-- test component in isolation and efficiently test complex interaction, e.g., without using high-fidelity simulators. -->
- Employing narrow integration tests and contract tests with fakes and mocks and adding them to continuous integration to test the integration of components built by individual teams.
- Investing in an initial version of physical continuous integration or hardware-in-the-loop tests to improve the fidelity of end-to-end testing.
- Improving deployment tools, e.g., utilizing container orchestrator.
- Structuring logs and setting up a log aggregator to track performance, enabling a quick check of things.
- Implementing an initial version of record and replay functionality, along with some sort of data visualization to aid in debugging.

### Series C: 201-2000 employees

**Key Practices:**

- Testing
    - Faster testing, e.g., via [TIA](https://martinfowler.com/articles/rise-test-impact-analysis.html)/test selection
    - Improving build time
    - Enhancing HIL or physical CI/CD
- Observability
    - Using log and data aggregators
- Debugging support
    - Enhancing record and replay, e.g., supporting jumping
    - Faster relaunch

### Beyond 2000 employees

Some companies at this stage seem to use custom hardware to accelerate computations unique to their needs.

## Closing notes

In this post, I have listed the challenges of testing robotics systems in fast-paced startups, shared my techniques for getting started with testing work, and provided insights on key practices for ensuring testability as an organization grows.

Let me know what you think by leaving comments below or messaging me on LinkedIn or Twitter!

- Do my experiences/approaches resonate/align (or not resonate/align) with yours?
- Do you have any test-related war stories or effective testing strategies you'd like to share?

I'd love to hear your thoughts.


<br>

#### Significant Revisions

- _2023/06/29_: Added the "Ensuring reliability as an organization grows" section
- _2023/05/28_: Rewrote the whole post

#### Footnotes

[^1] The identified challenges and my approaches may not generalize to other settings, such as testing in robotics companies that are much smaller (i.e., < 10 employees) or much bigger (i.e., > 1000 employees), or involving a different product, such as an autonomous vehicle-based ride-hailing service or an autonomous-based inspection service. For example, I don't have much experience with testing robotics systems that make heavy use of [machine learning](https://getcruise.com/news/blog/2020/cruises-continuous-learning-machine-predicts-the-unpredictable-on-san/) or [real-time programming](https://docs.ros.org/en/iron/index.html).
<br>[^2] See also [The Rise of Test Impact Analysis](https://martinfowler.com/articles/rise-test-impact-analysis.html) by Martin Fowler.
<br>[^3] E.g., see [ROSCon 2017 Vancouver Day 2 Determinism in ROS](https://youtu.be/II8yCw5tPE0) and [ROSCON 2019 MACAU: CONCURRENCY IN ROS 1 AND 2: FROM ASYNCSPINNER TO MULTITHREADEDEXECUTOR](https://vimeopro.com/osrfoundation/roscon-2019/video/379127709).
<br>[^4] For code examples, see [Stateful testing](https://hypothesis.readthedocs.io/en/latest/stateful.html) (Python) or [Detecting the unexpected in (Web) UI](https://medium.com/criteo-engineering/detecting-the-unexpected-in-web-ui-fuzzing-1f3822c8a3a5) (JavaScript).
<br>[^5] Check out [IntegrationTest](https://martinfowler.com/bliki/IntegrationTest.html) by Martin Fowler for related discussion, e.g., about narrow and broad integration tests.
<br>[^6] Check out [On the Diverse And Fantastical Shapes of Testing](https://martinfowler.com/articles/2021-test-shapes.html) (at least the last paragarph starting with "If you're paying my careful prose ...") by Martin Fowler for related discussion.
<br>[^7] I enjoy following research papers in this space, such as those taking a grammar-based approach like ["Scenic: a language for scenario specification and scene generation"](https://dl.acm.org/doi/abs/10.1145/3314221.3314633).
<br>[^8] See also [this twit thread](https://twitter.com/GergelyOrosz/status/1665340939529773057) from Gergely Orosz.
<br>[^9] See also my another post [Robo-Observability](https://mjyc.github.io/2023/04/21/observability.html).
