---
layout: post
title: "Testing robotics systems in fast-moving organizations"
date: 2020-12-16 00:00:00 -0800
updated: 2020-05-28 00:00:00 -0800
tags: ["#software-engineering"]
comments: true
---

<figure>
  <img src="https://live.staticflickr.com/195/506281600_a68f821d33_c.jpg" width="480px">
  <figcaption>Starcraft II, Photo by <a href="https://www.flickr.com/photos/tirrell/">Zach Tirrell</a> on <a href="https://www.flickr.com/">flickr</a></figcaption>
</figure>

Testing robotics systems is hard.
Based on my limited experience working at startups with fewer than 200 employees and fewer than 100 robots providing RaaS using fleets of indoor mobile robots or lines of robot manipulators, the main reasons for the difficulty were as follows:
1. _Edge cases and corner cases in production environments._
1. _The difficulty of using simulation._
1. _Challenges with adopting automation._

To address some of these challenges, I've developed the following approaches[^1]:
1. _Prioritization framework for creating tests._
1. _Procedure for identifying what to test._

## Why is testing robotics systems hard?

In production, I've encountered various edge cases and corner cases:

- _Unexpected peak load condition/usage pattern._
    It is common for multiple (custom) software, such as core robotics, monitoring, and infra-related software, to run in parallel.
    Unexpected high demands can adversarially impact your program, e.g., by consuming all of the available resources.
    Anticipating and recreating such situations is challenging, especially when dealing with (custom) software at all levels, including firmware and system software.
- _Edge cases for robotics algorithms._
    Input spaces for robotics algorithms, such as perception, control, and motion planning, are vast and challenging to effectively cover for edge cases; there is always a specific layout that causes navigation failures or a particular scene with specific objects that leads to grasping failures.
    Characterizing such instances is difficult and algorithm-dependent, which complicates the testing setup.
- _Rare hardware issues._
    Rare hardware issues that are not (directly) detectable are the worst, such as a small damage in the robot cell structure that requires adjusting the collision map.
    Anticipating such issues requires input from domain experts (e.g., mechanical or firmware engineers), who may not be easily accessible and speak different jargons and reproducing them often requires changing interfaces, which can be expensive (e.g., it becomes yet another layer to maintain).
- _Subtle regression._
    The complexity of robotics systems makes it challenging to establish a robust [regression testing](https://katalon.com/resources-center/blog/regression-testing) pipeline.
    For example, handling low-frequency [flaky tests](https://docs.gitlab.com/ee/development/testing_guide/flaky_tests.html), implementing robust [test selection and prioritization](https://damorimrg.github.io/practical_testing_book/testregression/selectionprio.html) is difficult and hence elusive bugs slip back into the production code.
    Performance regressions are particularly challenging, as they are subtle and require expensive measures such as repeated end-to-end tests and delicate statistical methods to detect.

Using simulation for testing robotics systems effectively is not as easy as it seems.

- _Inadequate usage._
    I find that simulation testing is most useful for end-to-end testing of robot applications.
    However, I often encounter test cases that would benefit from using other tools and techniques (e.g., for efficiency).
    All too frequently, I come across test cases for robot behaviors (e.g., implemented in finite state machine or behavior tree) that use simulation, making the test cases much more expensive than they need to be.
    In such cases, using alternatives like [fake](https://martinfowler.com/bliki/TestDouble.html) or [model-based testing](https://www.educative.io/answers/what-is-model-based-testing)[^2] would be much more efficient, as they can be used to only "simulate" the directly relevant modules[^3].
    This often stems from organizational issues, such as unclear boundaries between teams that result in poorly defined interfaces and testing strategies or more typical insufficient allocation of time for testing/addressing technical debts (e.g., in favor of prioritizing other deliverables).
- _Generating test cases._
    Even with simulation libraries that provide high-level interfaces for building scenarios, creating effective test scenarios is challenging.
    Creating a single simulated environment for end-to-end testing alone is laborious enough, so diversifying the test scenarios (e.g., to cover extreme cases) becomes a nice-to-have[^4].
    There are commercial products that address this issue (e.g., [AWS RoboMaker WorldForge](https://aws.amazon.com/blogs/aws/aws-announces-worldforge-in-aws-robomaker/)), but they are not easy for smaller organizations to integrate due to reasons such as integration cost, vendor lock-in, etc.
- _Expressing specifications._
    Specifications for many robotics programs, e.g., those involving perception, motion planning, and behaviors, are difficult to express due to their spatiotemporal nature.
    This leads to verbose and unorganized (e.g., containing duplicates) test code, which makes it difficult to maintain and scale.
- _Managing infrastructure._
    I haven't met a single person who loves managing simulation testing infrastructure, e.g., for continuous integration.
    Simulation test code is expensive to run, requires special hardware such as GPUs, and is difficult to optimize and move around (e.g., in cloud environments).
    This leads to a poor developer experience and can even result in the disabling of simulation testing.

Automating robotics software testing is still hard.

- _Challenges with automating build and deployment._
    Here are tech talks and a blog post that shed light on this topic:
    - ["Building Self Driving Cars with Bazel"](https://youtu.be/fjfFe98LTm8) from Cruise, BazelCon 2019 - shares Cruise's experiences with building and testing robotics software at scale
    - ["The landscape of software deployment in robotics"](https://www.airbotics.io/blog/software-deployment-landscape) from Airbotics - summarizes the typical challenges with deploying robotics software
    - ["Physical continuous integration on real robots"](https://youtu.be/JNV9CkARh_g) from Fetch, ROSCon 2016 - shares Fetch's experience with setting up and using a physical continuous integration pipeline
    - ROS (2) docs on Build (tools): [catkin/conceptual_overview](http://wiki.ros.org/catkin/conceptual_overview), [A universal build tool](https://design.ros2.org/articles/build_tool.html), [About the build system](https://docs.ros.org/en/iron/Concepts/About-Build-System.html)
    - ROS (2) docs on Deployment (tools): [Deployment Guidelines](https://docs.ros.org/en/iron/Tutorials/Advanced/Security/Deployment-Guidelines.html), [bloom](http://wiki.ros.org/bloom)
- _No standard._
    Automating the testing of software requires agreements among engineering teams on build, deployment, and test models.
    Given how robotics brings multiple communities together, such as research (e.g., computer vision, robotics), web development (e.g., frontend, backend), DevOps, embedded, etc., reaching such an agreement, or even discussing ideas (e.g., due to different backgrounds), is difficult.
    While the Robot-Operating System (ROS) and the communities around it have made significant progress in this regard, the lack of standards still seems to be a significant problem in organizations.

## Where to start

In fast-paced robotics startups that build complex systems, creating comprehensive test suites that cover all major failure scenarios is impossible.
To produce high-impact tests within the time budget, I use my adopted [Eisenhower Matrix](https://www.eisenhower.me/eisenhower-matrix/) to prioritize a list of failure scenarios by first categorizing (potential) failure scenarios according to their (expected) frequency and risk.

<figure>
  <img src="/assets/imgs/mcmatrix.png" width="480px">
</figure>

1. _First Quadrant (upper left): frequent and high-risk._
    In Quadrant 1 (Q1), I place failure scenarios that need to be covered immediately, e.g., that I hear all the time from internal communication channels, such as introducing breaking changes to APIs and dependencies.
1. _Second Quadrant (upper right): frequent and medium-risk._
    In Quadrant 2 (Q2), I place failure scenarios that occur frequently but allow continued operations with short downtime like unreliable hardware or unresponsive user interface issues with well-established alerts and recovery procedures.
1. _Third Quadrant (lower left): infrequent and high-risk._
    In Quadrant 3 (Q3), I place failure scenarios that occur rarely but causes significant disruption in operations such as core robotics component failure scenario or unexpected peak usage pattern.
1. _Fourth Quadrant (lower right): infrequent and medium-risk._
    In Quadrant 4 (Q4), I place failure scenarios that occur relatively infrequently and allow continued operations.

The placement of example failure scenarios in quadrants will differ across companies.
For instance, depending on the maturity of the robot product/prototype or the amount of time invested by the engineering team in designing the system, an unreliable hardware failure scenario may belong in Q1 (e.g., if it is causing multiple issues) or a robotics algorithms failure scenario may belong in Q2 (e.g., if the failure is not catastrophic or easily recoverable).
In general, I create or improve tests for one quadrant at a time, in increasing order.
After working on tests for Q1, I move on to tests for Q2 before addressing those for Q3.
This is because creating tests for Q3 requires a significant time investment, for example, to ensure the reproducibility of the failure.
Usually, there is no time available to work on tests for Q4.

So far, I have assumed that the failure scenarios to test are known; however, this is usually not the case. To determine what to test, I follow these steps:

1. _Gain access to internal alerts, dashboards, and logs._
    Investigating recently reported problems or analyzing the latest trends using monitoring tools is the easiest way to identify high-risk failure scenarios.
    If monitoring tools are not set up (e.g., in smaller companies), getting involved in operations work is another way to uncover potential high-value tests to create.
1. _Identify interface and service boundaries._
    Understanding such boundaries, e.g., by searching for internal documentation and system diagrams or examining the codebase, enables the creation or improvement of contract tests and integration tests that prevent unhandled breaking changes.
1. _Identify loose/implicit dependencies._
    Edge cases such as crashed applications due to low resources or unexpected hardware states can be considered as dependencies that are not explicitly specified.
    Identifying as many implicit dependencies, e.g., minimum resource requirements or compatible hardware states, can help test system behaviors under extreme conditions.

## Closing notes

In this post, I haven't really discussed approaches to addressing the challenges associated with using simulations and adopting automations.
I don't yet have enough insights to form presentable approaches, but I plan to update the post as soon as I form some.


<br>

#### Footnotes

[^1] The identified challenges and my approaches may not work well in other settings, such as testing in robotics companies that are much smaller (i.e., < 10 employees) or much bigger (i.e., > 1000 employees), or involving a different product, such as an autonomous vehicle-based ride-hailing service or an autonomous-based inspection service.
<br>[^2] For code examples, see [Stateful testing](https://hypothesis.readthedocs.io/en/latest/stateful.html) (Python) or [Detecting the unexpected in (Web) UI](https://medium.com/criteo-engineering/detecting-the-unexpected-in-web-ui-fuzzing-1f3822c8a3a5) (JavaScript).
<br>[^3] [Martin Fowler](https://martinfowler.com/) makes a similar point in [IntegrationTest](https://martinfowler.com/bliki/IntegrationTest.html) (see "Using this combination of ...") and discusses a related issue in [On the Diverse And Fantastical Shapes of Testing](https://martinfowler.com/articles/2021-test-shapes.html), which is related to my point in the following sentence above ("This often stems ...").
<br>[^4] I enjoy following research papers in this space, such as those taking a grammar-based approach like ["Scenic: a language for scenario specification and scene generation"](https://dl.acm.org/doi/abs/10.1145/3314221.3314633).
