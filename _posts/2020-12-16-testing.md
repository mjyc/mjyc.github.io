---
layout: post
title: "Testing robotics systems"
date: 2020-12-16 00:00:00 -0800
updated: 2020-05-28 00:00:00 -0800
tags: ["#software-engineering"]
---

<figure>
  <img src="https://live.staticflickr.com/195/506281600_a68f821d33_c.jpg" width="480px">
  <figcaption>Starcraft II, Photo by <a href="https://www.flickr.com/photos/tirrell/">Zach Tirrell</a> on <a href="https://www.flickr.com/">flickr</a></figcaption>
</figure>

Testing robotics systems is hard.
Based on my experience working at startups with fewer than 200 employees and fewer than 100 robots providing RaaS involving fleets of indoor mobile robots or lines of robot manipulators, the main reasons for the difficulty are as follows:
1. _Edge cases and corner cases in production environments._
1. _The difficulty of using simulation._
1. _Challenges with adopting automation._

To address some of these challenges, I've developed the following strategies:
1. _Looking for loose/implicit contracts._
1. _Tracking interface and service boundaries._
1. _(Re)designing with automation in mind._

Let's dive deeper into these[^1].

## Why is testing robotics systems hard?

In production, I've encountered various edge cases and corner cases:

- _Unexpected peak load condition/usage pattern._
    It is common for multiple (custom) software, such as core robotics, monitoring, and infra-related software, to run in parallel.
    Unexpected high demands can adversarially impact your program, e.g., by consuming all of the available resources.
    Anticipating and recreating such situations is challenging, especially when dealing with (custom) software at all levels, including firmware and system software.
- _Edge cases for robotics algorithms._
    Input spaces for robotics algorithms, such as perception, control, and motion planning, are vast and challenging to effectively cover for edge cases; there is always a specific layout that causes navigation failures or a particular scene with specific objects that leads to grasping failures.
    Characterizing such instances is difficult and algorithm-dependent, which complicates the testing setup.
- _Rare hardware issues._
    Rare hardware issues that are not (directly) detectable are the worst, such as a small damage in the robot cell structure that requires adjusting the collision map.
    Anticipating them requires input from domain experts (mechanical or firmware engineers who may not be easily accessible and speak different jargons), and reproducing them often requires changing interfaces, which can be expensive (becomes yet another layer to maintain).
- _Subtle regression._
    The complexity of robotics systems makes it challenging to establish a robust [regression testing](https://katalon.com/resources-center/blog/regression-testing) pipeline.
    For example, handling low-frequency [flaky tests](https://docs.gitlab.com/ee/development/testing_guide/flaky_tests.html), implementing robust [test selection and prioritization](https://damorimrg.github.io/practical_testing_book/testregression/selectionprio.html) is difficult and hence elusive bugs slip back into the production code.
    Performance regressions are particularly challenging, as they are subtle and require expensive measures such as repeated end-to-end tests and delicate statistical methods to detect.

Using simulation for testing robotics systems effectively is not as easy as it seems.

- _Inadequate usage._
    I find that simulation testing is most useful for end-to-end testing of robot applications.
    However, I often encounter test cases that would benefit from using other tools and techniques (e.g., for efficiency).
    All too frequently, I come across test cases for robot behaviors (e.g., implemented in finite state machine or behavior tree) that use simulation, making the test cases much more expensive than they need to be.
    In such cases, using alternatives like [fake](https://martinfowler.com/bliki/TestDouble.html) or [model-based testing](https://www.educative.io/answers/what-is-model-based-testing)[^2] would be much more efficient, as they can be used to only "simulate" the directly relevant modules[^3].
    This often stems from organizational issues, such as unclear boundaries between teams that result in poorly defined interfaces and testing strategies or more typical insufficient allocation of time for testing/addressing technical debts (e.g., in favor of prioritizing other deliverables).
- _Generating test cases._
    Even with simulation libraries that provide high-level interfaces for building scenarios, creating effective test scenarios is challenging.
    Creating a single simulated environment for end-to-end testing alone is laborious enough, so diversifying the test scenarios (e.g., to cover extreme cases) becomes a nice-to-have[^4].
    There are commercial products that address this issue (e.g., [AWS RoboMaker WorldForge](https://aws.amazon.com/blogs/aws/aws-announces-worldforge-in-aws-robomaker/)), but they are not easy for smaller organizations to integrate due to reasons such as integration cost, vendor lock-in, etc.
- _Expressing specifications._
    Specifications for many robotics programs, e.g., those involving perception, motion planning, and behaviors, are difficult to express due to their spatiotemporal nature.
    This leads to verbose and unorganized (e.g., containing duplicates) test code, which makes it difficult to maintain and scale.
- _Managing infrastructure._
    I haven't met a single person who loves managing simulation testing infrastructure, e.g., for continuous integration.
    Simulation test code is expensive to run, requires special hardware such as GPUs, and is difficult to optimize and move around (e.g., in cloud environments).
    This leads to a poor developer experience and can even result in the disabling of simulation testing.

Automating robotics software testing is still hard.

- _Challenges with automating build and deployment._
    Here are tech talks and a blog post that shed light on this topic:
    - ["Building Self Driving Cars with Bazel"](https://youtu.be/fjfFe98LTm8) from Cruise, BazelCon 2019 - shares Cruise's experiences with building and testing robotics software at scale
    - ["The landscape of software deployment in robotics"](https://www.airbotics.io/blog/software-deployment-landscape) from Airbotics - summarizes the typical challenges with deploying robotics software
    - ["Physical continuous integration on real robots"](https://youtu.be/JNV9CkARh_g) from Fetch, ROSCon 2016 - shares Fetch's experience with setting up and using a physical continuous integration pipeline
    - ROS (2) docs on Build (tools): [catkin/conceptual_overview](http://wiki.ros.org/catkin/conceptual_overview), [A universal build tool](https://design.ros2.org/articles/build_tool.html), [About the build system](https://docs.ros.org/en/iron/Concepts/About-Build-System.html)
    - ROS (2) docs on Deployment (tools): [Deployment Guidelines](https://docs.ros.org/en/iron/Tutorials/Advanced/Security/Deployment-Guidelines.html), [bloom](http://wiki.ros.org/bloom)
- _No standard._
    Automating the testing of software requires agreements among engineering teams on build, deployment, and test models.
    Given how robotics brings multiple communities together, such as research (e.g., computer vision, robotics), web development (e.g., frontend, backend), DevOps, embedded, etc., reaching such an agreement, or even discussing ideas (e.g., due to different backgrounds), is difficult.
    While the Robot-Operating System (ROS) and the communities around it have made significant progress in this regard, the lack of standards still seems to be a significant problem in organizations.

## What do we do?

> The remainder of this post is a work in progress.
> Please check back for updates! 

Here are my strategies.

### Unit and property-based testing

Robotics systems often consist of layers of application programming interfaces (APIs).
While it is difficult to test the lowest-level API like a ROS hardware driver, but everything above can be tested by simulating responses from a lower-layer API using any unit testing framework such as unittest (python), jest (javascript), gtest (C++).
For example, one could test perception algorithms like tabletop pose detector by simulating input images or point clouds via previously stored data (e.g., benchmark data) or algorithmically generated data.
The testing approach of algorithmically generating data is interesting because it allows [property-based testing](https://medium.com/criteo-labs/introduction-to-property-based-testing-f5236229d237), i.e., generating test cases[^1][^2].
This requires designing (simple) environment generation algorithms but one could simply use existing algorithms like the ones available in [PythonRobotics](https://atsushisakai.github.io/PythonRobotics/).
The key ideas are (1) testing small and independent layers of APIs using (2) algorithmically generated inputs that one could also easily compute expected outputs.

[^1] inspired by model-based UI testing frameworks <br>
[^2] Zhoulai Fu and Francisco Martinez Lasaca gave a talk "Experiences with Fuzz Testing ROS Component" that covered a similar approach "fuzz testing" and shared [their code](https://ros2-fuzzer.readthedocs.io/en/latest/)

### Sequential property-based testing

Many robotics algorithms work with sequential data.
For example, think of state estimation, control, motion planning algorithms.
Out of the box, property-based testing tools do not provide creating sequential data.
So the first step to testing time-dependent algorithm is updating property-based testing tools to be able to generate sequential random data.
Creating a sequential sampler that does not dependent on the outputs of testing algorithms is easy (e.g., inputs for state estimation algorithms) but otherwise, one requires to create reactive environments using existing tools like combinators.
At this point, basically one requires to build a tiny simulation environment.
While this sounds daunting, it's actually not thanks to many existing physics simulation libraries and game programming design patterns (e.g., entity-component systems, etc.).

The second problem is verifying test cases over time.
For example, one might want to check whether the outputs of a state estimation algorithm ever return `null` or test if a motion planning algorithm ever returns an invalid pose.
I like to use [linear temporal logic (LTL)](https://en.wikipedia.org/wiki/Linear_temporal_logic) to verify such temporal properties due to the LTL's simplicity.
There are some subtleties of using LTLs with property-based testing tools and I'll elaborate on those in the future (if I find more time).

### Physical integration testing

Like everything robotics, one must test programs with the real robots in real world at some point.
One approach is integrating continuous integration testing with real robots.
There was [a great talk](https://roscon.ros.org/2016/presentations/PhysicalContinuousIntegrationSlides.pdf) about this topic from fetch regarding this topic.
[A more recent talk about AWS Robomaker](https://youtu.be/SzHw2PIEIKQ) also touches on this topic, too.

## Closing meme

Testing is important but being ultra-selective about which code to add to a repository is also important because maintenance is hard.

<figure>
  <img src="https://raw.githubusercontent.com/dominictarr/push-streams-talk/master/meme.png">
  <figcaption>source: https://github.com/dominictarr/push-streams-talk</figcaption>
</figure>


<br>
<hr>

[^1] The identified challenges and my strategies may not work well in other settings, such as testing in robotics companies that are much smaller (i.e., < 10 employees) or much bigger (i.e., > 1000 employees), or involving a different product, such as an autonomous vehicle-based ride-hailing service or an autonomous-based inspection service.
<br>[^2] For code examples, see [Stateful testing](https://hypothesis.readthedocs.io/en/latest/stateful.html) (Python) or [Detecting the unexpected in (Web) UI](https://medium.com/criteo-engineering/detecting-the-unexpected-in-web-ui-fuzzing-1f3822c8a3a5) (JavaScript).
<br>[^3] [Martin Fowler](https://martinfowler.com/) makes a similar point in [IntegrationTest](https://martinfowler.com/bliki/IntegrationTest.html) (see "Using this combination of ...") and discusses a related issue in [On the Diverse And Fantastical Shapes of Testing](https://martinfowler.com/articles/2021-test-shapes.html), which is related to my point in the following sentence above ("This often stems ...").
<br>[^4] I enjoy following research papers in this space, such as those taking a grammar-based approach like ["Scenic: a language for scenario specification and scene generation"](https://dl.acm.org/doi/abs/10.1145/3314221.3314633).
