---
layout: post
title: "Testing robotics systems in fast-moving organizations"
date: 2020-12-16 00:00:00 -0800
updated: 2020-05-28 00:00:00 -0800
tags: ["#software-engineering"]
published: false
---

<figure>
  <img src="https://live.staticflickr.com/195/506281600_a68f821d33_c.jpg" width="480px">
  <figcaption>Starcraft II, Photo by <a href="https://www.flickr.com/photos/tirrell/">Zach Tirrell</a> on <a href="https://www.flickr.com/">flickr</a></figcaption>
</figure>

Testing robotics systems is hard.
Based on my limited experience working at startups with fewer than 200 employees and fewer than 100 robots providing RaaS using fleets of indoor mobile robots or lines of robot manipulators, the main reasons for the difficulty were as follows:
1. _Edge cases and corner cases in production environments._
1. _The difficulty of using simulation._
1. _Challenges with adopting automation._

To address some of these challenges, I've developed the following strategies:
1. _Looking for loose/implicit contracts._
1. _Tracking interface and service boundaries._
1. _(Re)designing with automation in mind._

Let's dive deeper into these[^1].

## Why is testing robotics systems hard?

In production, I've encountered various edge cases and corner cases:

- _Unexpected peak load condition/usage pattern._
    It is common for multiple (custom) software, such as core robotics, monitoring, and infra-related software, to run in parallel.
    Unexpected high demands can adversarially impact your program, e.g., by consuming all of the available resources.
    Anticipating and recreating such situations is challenging, especially when dealing with (custom) software at all levels, including firmware and system software.
- _Edge cases for robotics algorithms._
    Input spaces for robotics algorithms, such as perception, control, and motion planning, are vast and challenging to effectively cover for edge cases; there is always a specific layout that causes navigation failures or a particular scene with specific objects that leads to grasping failures.
    Characterizing such instances is difficult and algorithm-dependent, which complicates the testing setup.
- _Rare hardware issues._
    Rare hardware issues that are not (directly) detectable are the worst, such as a small damage in the robot cell structure that requires adjusting the collision map.
    Anticipating such issues requires input from domain experts (e.g., mechanical or firmware engineers), who may not be easily accessible and speak different jargons and reproducing them often requires changing interfaces, which can be expensive (e.g., it becomes yet another layer to maintain).
- _Subtle regression._
    The complexity of robotics systems makes it challenging to establish a robust [regression testing](https://katalon.com/resources-center/blog/regression-testing) pipeline.
    For example, handling low-frequency [flaky tests](https://docs.gitlab.com/ee/development/testing_guide/flaky_tests.html), implementing robust [test selection and prioritization](https://damorimrg.github.io/practical_testing_book/testregression/selectionprio.html) is difficult and hence elusive bugs slip back into the production code.
    Performance regressions are particularly challenging, as they are subtle and require expensive measures such as repeated end-to-end tests and delicate statistical methods to detect.

Using simulation for testing robotics systems effectively is not as easy as it seems.

- _Inadequate usage._
    I find that simulation testing is most useful for end-to-end testing of robot applications.
    However, I often encounter test cases that would benefit from using other tools and techniques (e.g., for efficiency).
    All too frequently, I come across test cases for robot behaviors (e.g., implemented in finite state machine or behavior tree) that use simulation, making the test cases much more expensive than they need to be.
    In such cases, using alternatives like [fake](https://martinfowler.com/bliki/TestDouble.html) or [model-based testing](https://www.educative.io/answers/what-is-model-based-testing)[^2] would be much more efficient, as they can be used to only "simulate" the directly relevant modules[^3].
    This often stems from organizational issues, such as unclear boundaries between teams that result in poorly defined interfaces and testing strategies or more typical insufficient allocation of time for testing/addressing technical debts (e.g., in favor of prioritizing other deliverables).
- _Generating test cases._
    Even with simulation libraries that provide high-level interfaces for building scenarios, creating effective test scenarios is challenging.
    Creating a single simulated environment for end-to-end testing alone is laborious enough, so diversifying the test scenarios (e.g., to cover extreme cases) becomes a nice-to-have[^4].
    There are commercial products that address this issue (e.g., [AWS RoboMaker WorldForge](https://aws.amazon.com/blogs/aws/aws-announces-worldforge-in-aws-robomaker/)), but they are not easy for smaller organizations to integrate due to reasons such as integration cost, vendor lock-in, etc.
- _Expressing specifications._
    Specifications for many robotics programs, e.g., those involving perception, motion planning, and behaviors, are difficult to express due to their spatiotemporal nature.
    This leads to verbose and unorganized (e.g., containing duplicates) test code, which makes it difficult to maintain and scale.
- _Managing infrastructure._
    I haven't met a single person who loves managing simulation testing infrastructure, e.g., for continuous integration.
    Simulation test code is expensive to run, requires special hardware such as GPUs, and is difficult to optimize and move around (e.g., in cloud environments).
    This leads to a poor developer experience and can even result in the disabling of simulation testing.

Automating robotics software testing is still hard.

- _Challenges with automating build and deployment._
    Here are tech talks and a blog post that shed light on this topic:
    - ["Building Self Driving Cars with Bazel"](https://youtu.be/fjfFe98LTm8) from Cruise, BazelCon 2019 - shares Cruise's experiences with building and testing robotics software at scale
    - ["The landscape of software deployment in robotics"](https://www.airbotics.io/blog/software-deployment-landscape) from Airbotics - summarizes the typical challenges with deploying robotics software
    - ["Physical continuous integration on real robots"](https://youtu.be/JNV9CkARh_g) from Fetch, ROSCon 2016 - shares Fetch's experience with setting up and using a physical continuous integration pipeline
    - ROS (2) docs on Build (tools): [catkin/conceptual_overview](http://wiki.ros.org/catkin/conceptual_overview), [A universal build tool](https://design.ros2.org/articles/build_tool.html), [About the build system](https://docs.ros.org/en/iron/Concepts/About-Build-System.html)
    - ROS (2) docs on Deployment (tools): [Deployment Guidelines](https://docs.ros.org/en/iron/Tutorials/Advanced/Security/Deployment-Guidelines.html), [bloom](http://wiki.ros.org/bloom)
- _No standard._
    Automating the testing of software requires agreements among engineering teams on build, deployment, and test models.
    Given how robotics brings multiple communities together, such as research (e.g., computer vision, robotics), web development (e.g., frontend, backend), DevOps, embedded, etc., reaching such an agreement, or even discussing ideas (e.g., due to different backgrounds), is difficult.
    While the Robot-Operating System (ROS) and the communities around it have made significant progress in this regard, the lack of standards still seems to be a significant problem in organizations.

## What do we do?

Due to the complexity of robotics systems--and hence myriad ways they can fail, it is difficult to figure out where to get started.
Whenever I start work on a new codebase, it isn't clear what test case to write first.
In such cases, use this matrix to prioritize the possible tasks.

<link rel="shortcut icon" type="image/png" href="/assets/imgs/favicon.svg">

1. _First Quadrant (upper right): frequent and high value._
    Examples tasks in this quadrant includes testing for breaking changes (e.g., interface)
    - Examples:
        - Breaking contract
        - Robotics algorithm (if the team is actively developing the feature)
    - Justification:
        - The most important
    - How to spot:
        - You will hear about it, from every where
1. Quadrant 2 - Frequent & Low impact
    - Examples:
        - Robotics algorithms
    - Justification
        - the most important
    - How to spot:
        - You will encounter it, and ask about it, they brush shoulder for you
        - Ask about it
        - If there's tons of notes, it's more like 1 but they moved on
        - Important to justify
1. Quadrant 3 - Rare & High impact
    - Examples:
        - Rare hardware issues
        - Unexpected peak load
    - Justification
        - High impact
    - How to spot:
        - You'll hear about it, from like drinking or something
1. Quadrant 4 - Rare & Low impact
    - Forget about this

I always feel like so many things can go wrong 

So we want to write high-impact test code within (time) budget.
Here is my approach.
<!-- This matrix summarizes my approach. -->

Step1: organize possible bugs/test to write into 

1. First Quadrant: Frequent & High impact quadrant._
    - Examples:
        - Breaking contract
        - Robotics algorithm (if the team is actively developing the feature)
    - Justification:
        - The most important
    - How to spot:
        - You will hear about it, from every where
1. Quadrant 2 - Frequent & Low impact
    - Examples:
        - Robotics algorithms
    - Justification
        - the most important
    - How to spot:
        - You will encounter it, and ask about it, they brush shoulder for you
        - Ask about it
        - If there's tons of notes, it's more like 1 but they moved on
        - Important to justify
1. Quadrant 3 - Rare & High impact
    - Examples:
        - Rare hardware issues
        - Unexpected peak load
    - Justification
        - High impact
    - How to spot:
        - You'll hear about it, from like drinking or something
1. Quadrant 4 - Rare & Low impact
    - Forget about this



<!-- (Mike's Matrix) -->


describe

<!-- Here is how I categorize and prioritize my tasks. -->

<!-- When it cofmes to testing, m y goal is to write the highest value test code within (time) budget.
When it goes to writing test code, my goal is to find and test the "hot spot" within (time) budget. -->




```
note
- add the link to grammar post in one of the footnotes in testing robotics
- move around citations to martin fowler

## What can we do?

You want to write high-value test code within budget.
Here is how I prioritize my effort.
<!-- that capture high impact and/or frequent problem on budgeted time. -->
<!-- Your manager don't give enough time to work on testing, here is how I determine what to tackle first. -->

<!-- note:
- this section is about  -->
<!-- - this section is about how to plan working on testing, e.g., deciding how to prioritize what test code to write and when -->
<!-- important to mention how to control amount of time you are putting in these -->

I like to think about value of writing test code and when to work on writing/improving test code (just testing?).



<!-- note:
- now I know what are high-value testing and when to work on them
- what do I actually do?
- HERE ARE the steps I take in organization (when building a new one, jump right into 4. but never really had such chance, you always face constraints)
 -->
<!-- Most of times, you are thrown to existing codebase and needs to maintain & improve. -->

Here is my strategy / steps:

1. Subscribe to alerts
    - things to do
        - getting access, etc.
    - things to check
        - enough monitoring?
        - data available & accessible & how easy?
            - centralized
            - query
    - note
        - mention this will help catching XYZ
1. Identify interface & service boundaries
    - things to do
        - diagramming
        - test pipeline
    - things to check
        - do they have strategies for contract testing?
            - in runtime/deployment time? (possible empirically)
        - ...
    - anticipate changes
        - upgrade
1. Identify loose/implicit dependencies
    - things to check
        - research history
        - test pipeline
        - think outside of box
    - anticipate problems
        - like bugs
        - with the current monitoring & testing solutions
            - low hanging fruit
            - go big go home
    - ask questions
        - provide example questions
1. Improve
    - ...

Ideas
- How to find hot spot/zone?
```


<br>
<hr>

[^1] The identified challenges and my strategies may not work well in other settings, such as testing in robotics companies that are much smaller (i.e., < 10 employees) or much bigger (i.e., > 1000 employees), or involving a different product, such as an autonomous vehicle-based ride-hailing service or an autonomous-based inspection service.
<br>[^2] For code examples, see [Stateful testing](https://hypothesis.readthedocs.io/en/latest/stateful.html) (Python) or [Detecting the unexpected in (Web) UI](https://medium.com/criteo-engineering/detecting-the-unexpected-in-web-ui-fuzzing-1f3822c8a3a5) (JavaScript).
<br>[^3] [Martin Fowler](https://martinfowler.com/) makes a similar point in [IntegrationTest](https://martinfowler.com/bliki/IntegrationTest.html) (see "Using this combination of ...") and discusses a related issue in [On the Diverse And Fantastical Shapes of Testing](https://martinfowler.com/articles/2021-test-shapes.html), which is related to my point in the following sentence above ("This often stems ...").
<br>[^4] I enjoy following research papers in this space, such as those taking a grammar-based approach like ["Scenic: a language for scenario specification and scene generation"](https://dl.acm.org/doi/abs/10.1145/3314221.3314633).

```
TODO: say see MArtin's IntegrationTest, for related discussions makes a similar point
```
