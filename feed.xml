<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2026-01-06T17:21:06+00:00</updated><id>/feed.xml</id><title type="html">Michael Jae-Yoon Chung</title><entry><title type="html">Robo-Observability</title><link href="/2023/04/21/observability.html" rel="alternate" type="text/html" title="Robo-Observability" /><published>2023-04-21T08:00:00+00:00</published><updated>2023-04-21T08:00:00+00:00</updated><id>/2023/04/21/observability</id><content type="html" xml:base="/2023/04/21/observability.html"><![CDATA[<p>I care about observability in the context of debugging and monitoring robotics systems.</p>

<h2 id="logs">Logs</h2>

<p>Debugging (e.g., ROS-based) robotics systems by digging through logs hasn’t been fun for me.
To investigate a bug report, I first SSH into a robot, look through multiple log files from multiple teams’ software, and once I find relevant logs and data (e.g., camera images), I start downloading them to my local dev machine and wait for 10~20 minutes (at best).
Only after that, I could dive deep into them to understand the reported issue.
I’m describing my ~worst experience, but debugging (and monitoring, too) robotics systems has always been painful for one or two reasons I just mentioned.</p>

<p>When I started using log management tools from the distributed systems community, I was pleasantly surprised by their amazing developer experience (DX).
Three aspects in particular stood out to me:</p>

<ul>
  <li><em>Structured logs</em> made logs machine-parsable and version controllable, eliminating the need for regex gymnastics.
  This greatly facilitated the development of user-facing tools like interactive data visualizers.</li>
  <li><em>Centralized logs</em> helped bring together all relevant information for users.</li>
  <li><em>Log visualization tools</em> allowed users to effortlessly navigate and process large amounts of log data.</li>
</ul>

<p>During the adoption of these aspects, I observed the following challenges faced by organizations:</p>

<ul>
  <li><em>Large and complex codebases</em> made it difficult and laborious to structure logs consistently across diverse subsystems.</li>
  <li><em>Large data volumes</em> posed challenges in centralizing data.
  Even in robotics companies that deal with non-autonomous vehicles, data generation reaches petabyte scales, making it incredibly challenging to work with.</li>
  <li><em>Non-textual logs</em> made the utilization of existing log management and visualization tools more difficult.</li>
</ul>

<p>Here are my suggestions:</p>
<ul>
  <li><strong>Start structuring logs by adding metadata</strong>  such as robot ID and customer ID (i.e., robot and customer information) to the logs of multiple teams.
  Doing so should spark discussions about standardizing the data structure and tooling for logs.
  Nudge stakeholders to think in terms of logs generated from fleets instead of individual robots, and manage the lifecycle of logs independently, for example, from the software that generated them.</li>
  <li>While aiming to standardize the metadata structure and tooling to simplify the consumption process, <strong>log data types and data channels</strong> carrying logs <strong>should be treated differently and separately to optimize performance</strong> in terms of transportation, visualization, and so on.</li>
  <li><strong>Invest in</strong> adopting or even building <strong>data visualization tools</strong>.
  “A picture is worth a thousand words.”
  Non-textual data is essential when it comes to debugging, and each organization may have bespoke needs.</li>
</ul>

<h2 id="metrics">Metrics</h2>

<p>Typical metric categories I’ve seen are:</p>

<ul>
  <li><em>Customer and robot-specific metrics</em>, such as the total number of completed deliveries and the total distance traveled for an indoor delivery robot company.</li>
  <li><em>Resource utilization and health-related metrics</em>, such as CPU, memory, and disk usage, network traffic of onboard and cloud machines.</li>
  <li><em>Service health and availability-related metrics</em>, such as request rate/error/duration, service uptime/response time of onboard and cloud services.</li>
</ul>

<p>These metrics aren’t specific to robotics companies and are standardized (e.g., across services) for ease of consumption and operational scalability.
However, I have found that specializing metrics for core robotics engineers (e.g., who also engage in operations work in smaller organizations) is helpful for monitoring purposes.
Here are examples of such specialized metrics for motion control and planning:</p>

<ul>
  <li><strong>Motion control:</strong> control frequency, number of staged controllers, response time of dependent hardware devices.</li>
  <li><strong>Motion planning:</strong> planning request rate, errors, and duration, distance and duration of planned motion (trajectory).</li>
</ul>

<p>Notice that these metrics are still high-level, i.e., general across different kinds of motion planning or control algorithms.
There are also <strong>robotics algorithm-specific metrics</strong> (e.g., the number of nodes explored for a sampling-based planner) that can be computed and tracked.
While I do like to collect such robotics-specific metrics to gain a deeper understanding of algorithm performance, doing so requires caution, e.g., I like to ask questions such as: What’s the overhead of computing algorithm-specific metrics? How can we extract meaningful information from these metrics and avoid adding noise to the dashboard? How much maintenance work do we anticipate?</p>

<h2 id="final-thoughts">Final Thoughts</h2>

<p>Observability is crucial for debugging and monitoring robotics systems at every stage of an organization.
In small organizations, observability is a must for quickly detecting and resolving issues.
For larger organizations, the ability to collect and process large log data from a fleet of robots effectively or monitor the health and utilization of such a fleet is a must.
Ensuring observability of a robotics system at scale requires not only careful design and nontrivial implementation work on tooling but also the establishment of conventions and practices that are agreed upon and adhered to by multiple teams.</p>

<p><br /></p>

<h4 id="acknowledgements">Acknowledgements</h4>

<p>I thank Chris Palmer and Rastislav Komara for sharing their experiences and insights.</p>]]></content><author><name></name></author><category term="#softwareengineering" /><summary type="html"><![CDATA[I care about observability in the context of debugging and monitoring robotics systems.]]></summary></entry><entry><title type="html">Transitioning from HRI research to the industry/Getting started with work</title><link href="/2022/05/30/transitioning.html" rel="alternate" type="text/html" title="Transitioning from HRI research to the industry/Getting started with work" /><published>2022-05-30T08:00:00+00:00</published><updated>2022-05-30T08:00:00+00:00</updated><id>/2022/05/30/transitioning</id><content type="html" xml:base="/2022/05/30/transitioning.html"><![CDATA[<figure>
  <img src="https://upload.wikimedia.org/wikipedia/commons/c/cf/Waymo_self-driving_car_front_view.gk.jpg" />
  <figcaption>A self driving car</figcaption>
</figure>

<h2 id="part-1">Part 1</h2>

<p><a href="/research">My research</a> in graduate school was on <a href="https://homes.cs.washington.edu/~todorov/courses/cseP590/07_HRI.pdf">human-robot interaction</a> (HRI), more specifically, on <a href="https://youtu.be/pTml6yEIjcw">end-user robot programming</a>.
Unlike other research fields like robotic perception or motion planning &amp; control, companies did not know what HRI was and weren’t sure how I could fit into their organization or which role I could take.
I realized that my colleagues who also focused on HRI had similar problems.</p>

<p>After spending some time in a couple of robotics startups and observing my colleagues’ career paths over the past couple of years, here is a list of roles that I think HRI researchers can take:</p>

<ul>
  <li>
    <p><strong>Front-end or Full-stack Engineer.</strong>
  Front-end engineers work on the user-facing part of the company’s software stack, while full-stack engineers work on all aspects of the (web) app stack.
  If you are an HRI researcher who has built end-to-end, user-facing robotics systems (e.g., for user studies, prototyping new designs/algorithms/systems, etc.), one of these roles may be a good fit.
  Another way to determine if one of these roles suits you is by exploring the communities surrounding the “industry standard” tools and technologies used by front-end and full-stack engineers[^1].
  Do you find the problems they are working on interesting? Can you envision yourself getting involved in these communities? If so, these roles might be the right fit for you.</p>

    <p>One note about front-end work: it usually involves working on screen-based user interfaces (UI), such as browser UI or mobile UI.
  In robotics companies, typical users of web or mobile apps are operators/robot wranglers (e.g., those who would benefit from fleet dashboards and teleop interfaces) or developers (e.g., those who would benefit from data visualization interfaces).
  Regarding full-stack work, it generally refers to software development for web applications.
  In my experience, the distinction between front-end and full-stack was blurry in smaller companies, as all members of the (web) application team worked on a little bit of everything.</p>
  </li>
  <li>
    <p><strong>Application Engineer.</strong>
  If you are an HRI researcher who enjoys building high-level robot behaviors, an application (or behavior or autonomy) engineer role may be a good fit.
  Application engineers are involved in designing, implementing, deploying, and maintaining robot applications (One note: the exact scope varies across organizations).
  Larger companies may have roles like sales or solutions engineer who closely work with customers, such as conducting demos or deploying/installing solutions.
  While I don’t have firsthand experience in a full time sales or solutions engineer role, I have observed other researchers in the general robotics field transitioning to these roles.
  Based on this, I strongly believe that HRI researchers who particularly enjoy prototyping or deploying systems and working closely with end-users may find these roles enjoyable.</p>
  </li>
  <li>
    <p><strong>UI/UX or Product designer.</strong>
  <a href="https://careerfoundry.com/en/blog/ui-design/what-does-a-ui-designer-actually-do/">UI</a>/<a href="https://careerfoundry.com/en/blog/ux-design/what-does-a-ux-designer-actually-do/">UX</a> designers are in charge of designing all visual elements and their interactive properties for web or mobile applications.
  <a href="https://www.productplan.com/glossary/product-designer/">Product designers</a> are responsible for the design process of the company’s product.
  In robotics companies, the scope of product design includes the robot’s form factor design, UI/UX design, and/or workflow design.
  Based on my observations, HRI researchers who have made contributions in the <a href="https://humanrobotinteraction.org/2023/full-papers/#themes">areas of “Design,” “Study,” or even “Theory”</a> tend to possess transferrable skills that align well with these roles.
  One pattern I have noticed is that those who have successfully transitioned to the industry seems to understand how companies evaluate candidates (and employees) and make a strong case for themselves.
  For instance, they place emphasis on their transferrable skills and showcase a track record of applying such skills in their research.
  Ashley Ruba explains this point well in <a href="https://www.linkedin.com/posts/ashleyruba-phd_careers-academia-resumes-activity-7016818268183175169-FI0N">this LinkedIn post</a>.</p>

    <p>While I may have made it sound like the product designer is a master-of-all designer (like <a href="https://newschoolarch.edu/blog/what-does-a-product-designer-do/">these</a> <a href="https://nulab.com/learn/design-and-ux/what-does-a-product-designer-do-and-how-is-it-different-from-ux-design/">articles</a>), in robotics companies, it appears that different types of designers require distinct training.
  For example, unlike how all web application engineers are essentially full-stack in smaller companies, I haven’t seen UI designers stepping up to do hardware design work too, which typically requires disciplines such as industrial design or mechanical engineering.
  Another related role is that of a user researcher, whose primary responsibility is gathering product requirements.
  In smaller companies, I have observed this role being fulfilled by a product or UX/UI designer.</p>
  </li>
  <li>
    <p><strong>Product Manager.</strong>
  This is an interesting one.
  <a href="https://uxdesign.cc/product-designer-vs-product-manager-whats-the-difference-anyway-1309c6a01ee9">Product managers are not product designers</a>.
  In my experience, product managers essentially herd the teams by identifying, organizing, and prioritizing tasks to get things done.
  I’m not sure what traits of HRI researchers succeed in this role.
  However, one thing I observed is that those who have a knack for business—-strangely, I’ve met some at HRI conferences—-seem to do well as product managers.</p>
  </li>
  <li>
    <p><strong>Quality Assurance Engineer or Tester.</strong>
  There are also interesting opportunities where HRI researchers can exercise their study design skills (e.g., for creating tests) and engineering skills (e.g., building tools for managing test software).</p>
  </li>
  <li>
    <p><strong>Robotic Perception, Motion Planning, or Control Engineer.</strong>
  More recently, I have seen more HRI researchers with strong core robotics skills.
  If this describes you, roles related to computer vision, machine learning, or core robotics may be a good fit.
  One thing I have noticed about HRI researchers is that they want to be involved in a little bit of everything (perhaps due to the interdisciplinary nature of HRI research) or have a strong desire to contribute to the whole system or system architecture.
  If this resonates with you, you can make a significant impact by getting involved in (or even leading, if you can) cross-team projects, which are common in smaller, nimble organizations.
  Additionally, you can consider working as an application engineer and collaborating with those who work in more specialized, core robotics teams.</p>
  </li>
  <li>
    <p><strong>HRI Researcher.</strong>
  This would be an obvious choice, but availability is limited.
  Also, the kind of HRI research work you do can vary greatly across companies.
  The related roles I’ve seen are usually focused on technical HRI research, such as working on new algorithms or computational methods supporting human-robot interaction.</p>
  </li>
</ul>

<p>I wish <a href="https://dl.acm.org/doi/10.5555/3523760.3523762">Leila Takayama’s HRI2022 keynote talk</a> is available somewhere.
She shared great advice on this topic in her keynote talk.
Actually, checkout her <a href="https://www.linkedin.com/in/leilatakayama/recent-activity/all/">LinkedIn post history</a>.
You will find relevant job postings.
Another blog post I like is <a href="https://foxglove.dev/blog/how-to-work-in-robotics-when-youre-not-a-roboticist">How to Work in Robotics When You’re Not a Roboticist</a> from <a href="https://foxglove.dev/">foxglove</a>, a robot data infrastructure and visualization company.
It offers another perspective on how to find a role you might like.
Finally, check out my <a href="https://github.com/mjyc/awesome-hri-papers-for-industry">awesome-hri-papers-for-industry</a>, which showcases HRI papers that demonstrate relevant research for the industry.</p>

<h2 id="part-2">Part 2</h2>

<p>After spending a long time in academia, working in the industry felt awkward.
Here are a few things I picked up for adjustment.</p>

<p>I realized that my goals in the industry are:</p>

<ol>
  <li>To get paid well and survive.</li>
  <li>To work on high-impact problems.</li>
  <li>To work effectively with my team and make each other more productive.</li>
</ol>

<p>First one is obvious (esp., when I’m a dad), second one is my desire which is also helpful for the first, and third one was a requirement for first two.</p>

<p>To achieve these goals, here are the latest steps I follow:</p>

<ol>
  <li><strong>Understand the organization.</strong>
Ask questions such as: Which team is in charge of what?
How do teams work together?
Understanding the org structure helps me focus on critical tasks for myself and my team, and become a better team player.</li>
  <li><strong>Understand how decisions are made.</strong>
Ask questions such as: Who are the decision makers?
Who are the other stakeholders?
In larger organizations, I want to know who creates my daily tasks and how.
Understanding the decision-making process helps me talk to the right person when pitching a project/initiative, seeking feedback, finding help, or when I disagree with certain tasks.</li>
  <li><strong>Understand the company’s business model.</strong>
Ask questions such as: How does my company (plan to) make money?
How do each team create business value?
Which teams are core to the (current or future) business?</li>
</ol>

<p>One habit I carried over from grad school is always preparing for a “defense” even when no such thing exists.
There are always suggestions for climbing the career ladder, but thinking in terms of qualification, general, and defense exams helps me understand exactly what I need to do.</p>

<ul>
  <li><strong>Qualification-like Phase:</strong> Get familiarized with the organization and tech stack, succeed in my first (assigned) project(s).</li>
  <li><strong>General-like Phase:</strong> Find ways to make a big contribution (e.g., finding a high-impact project, joining/contributing to a core project, forming/joining a strong team), and propose projects/initiatives or prototypes.</li>
  <li><strong>General-like Phase:</strong> Deliver and make an impact (e.g., support, maintain, grow).</li>
</ul>

<p>One habit I needed to change dramatically was the way I collaborate/work together with others.
In grad school, I found a way to collaborate based on mutual excitement for a research topic.
However, in the workplace, people’s motivations are different, and it’s not always apparent what their motivations are.
It became crucial to fully understand potential collaborators’ or teammates’ motivations (e.g., through social settings, small tasks, cross-team meetings, etc.) before embarking on forming a team or working together on a project.</p>

<p><br />
Let me know what your thoughts!</p>

<ul>
  <li>Was this post (not) useful for you? Which part was especially (not) useful?</li>
  <li>Do you have any insights on the topics discussed here? Any insights you strongly (dis)agree?</li>
  <li>Anything else?</li>
</ul>

<p>I’d love to hear them.</p>

<p><br /></p>

<h4 id="footnotes">Footnotes</h4>

<p>[^1] See <a href="https://roadmap.sh/frontend">Frontend</a> and <a href="https://roadmap.sh/backend">Backend</a> developer roadmaps to find the “industry standard” tools and technologies.</p>]]></content><author><name></name></author><category term="#productivity" /><category term="#gradschool" /><category term="#jobsearch" /><summary type="html"><![CDATA[[My research](/research) in graduate school was on [human-robot interaction](https://homes.cs.washington.edu/~todorov/courses/cseP590/07_HRI.pdf) (HRI), more specifically, on [end-user robot programming](https://youtu.be/pTml6yEIjcw).]]></summary></entry><entry><title type="html">In Search of the Grammar of Robot Applications</title><link href="/2021/12/29/search.html" rel="alternate" type="text/html" title="In Search of the Grammar of Robot Applications" /><published>2021-12-29T08:00:00+00:00</published><updated>2021-12-29T08:00:00+00:00</updated><id>/2021/12/29/search</id><content type="html" xml:base="/2021/12/29/search.html"><![CDATA[<figure>
  <img src="https://upload.wikimedia.org/wikipedia/commons/9/92/JOHN_COLTRANE.jpg" />
  <figcaption>A painting of <a href="https://en.wikipedia.org/wiki/John_Coltrane">John Coltrane</a></figcaption>
</figure>

<p>I’ve been searching for ways to make robot application programming easy[^1].
Here are my insights on the challenges in programming robot applications and my unfulfilled ideas for (dramatically) simplifying the process.</p>

<h2 id="challenges-with-using-existing-robot-behavior-representations">Challenges with using existing robot behavior representations</h2>

<p>In the past, I viewed a robot application as an application program that executes a (high-level) robot behavior.
The two most popular representations for implementing robot behaviors were (and still are) <a href="https://en.wikipedia.org/wiki/Finite-state_machine">finite state machine</a> (FSM) and <a href="https://en.wikipedia.org/wiki/Behavior_tree_(artificial_intelligence,_robotics_and_control)">behavior tree</a> (BT).
Thus, programming robot applications usually meant programming FSMs or BTs.
In theory, FSMs or BTs are straightforward to program.
They have well-defined semantics (e.g., mathematical definitions), which makes them predictable and reasonable, and they natively support composition, enabling the creation of complex behaviors by reusing simpler ones.</p>

<figure>
  <img src="http://wiki.ros.org/pr2_plugs_actions?action=AttachFile&amp;do=get&amp;target=smach.png" width="480px" />
  <figcaption>An FSM consists of <a href="http://wiki.ros.org/pr2_plugs_actions">pr2_plugs_actions</a></figcaption>
</figure>

<p>Programming robot applications with FSMs or BTs wasn’t so easy in practice.
The FSM and BT implementations I’ve met in the wild lacked clear semantics or gradually lost them over time, making them difficult to work with.
Frequently, I came across ill-defined transition functions that made it challenging to determine the triggers and conditions for transitions.
I often found side-effect-causing code snippets scattered across multiple states, used to force transitions or trigger actions instead of properly extending the transition function.
As a result, the FSMs became unpredictable and difficult to test.
Regarding BTs, I encountered subtleties specific to the implementation or programming language, e.g., in how the execution logic handles the “tick,” or misuses of BT features, e.g., abusing blackboard, that invariably led to complications[^2].</p>

<p>Creating complex behaviors wasn’t as easy as it appeared to be.
The most significant challenge I’ve faced was the maintenance of modules.
Driven by the composability of FSMs or BTs, developers often created intermediate FSM states or BT nodes (referred to as “modules”) to represent simple skills or patterns.
While developers intended these modules to be reusable, without proper planning, it was all too easy to end up with numerous modules that hindered the creation of complex behaviors.</p>

<figure>
  <img src="https://github.com/BehaviorTree/BehaviorTree.CPP/raw/master/docs/groot-screenshot.png" />
  <figcaption>A screenshot of <a href="https://github.com/BehaviorTree/BehaviorTree.CPP">BehaviorTree.CPP</a> editor</figcaption>
</figure>

<p>In most robotics (software) companies, the abovementioned challenges  weren’t insurmountable.
Given the utmost importance of maintaining robust robot applications, companies could readily allocate ample resources to ensure the predictability and composability of FSMs and BTs.
This could be achieved through extensive discussions, refactoring, testing, and more.</p>

<p>FSMs and BTs have been serving the robotics community well, and likely, they will continue to do so, and yet, I didn’t think using them was the future.
My primary concern with using FSMs and BTs was that they tend to nudge developers to view the robot as a solitary and central entity when determining the main flow of the application program.
In the RaaS companies I worked at, robot applications almost always involved multiple robots, such as fleets of indoor mobile robots or lines of robot manipulators.
Even when the application focused on the behavior of a single robot, the underlying system was a distributed one comprising robotics services (e.g., perception, control) and external services (e.g., user interface, scheduler).
Consequently, application developers needed to think in terms of multiple robots or services and their interactions.
In such a context, it made more sense to regard a robot application as a program that implements the orchestration logic of a distributed system.</p>

<h2 id="inspirations-from-non-robotics-communities">Inspirations from non-robotics communities</h2>

<figure>
  <img src="https://cycle.js.org/img/actuators-senses.svg" width="360px" />
  <figcaption><a href="https://cycle.js.org/dialogue.html">Dialogue abstraction</a>, Cycle.js</figcaption>
</figure>

<p>When reactive programming was making a buzz in the WebDev community, I stumbled upon <a href="https://cycle.js.org/">Cycle.js</a> and fell in love with it immediately.
I loved how Cycle.js’ language-agnostic core concepts, like <a href="https://dl.acm.org/doi/pdf/10.1145/258948.258973">functional reactive programming paradigm</a> and <a href="http://wiki.c2.com/?PortsAndAdaptersArchitecture">ports and adaptors architecture</a>, seemed to be transferrable to robot application programming.
I could imagine robot applications with complex interruption signals, e.g., consisting of external and internal signals like user input and system error signals, or more generally complex interaction flows, could be concisely expressed in a functional reactive programming paradigm.
By following ports and adaptors architecture, Cycle.js enforces a separation of side-effect-making code from the application logic, and it made testing a breeze, especially with <a href="https://rxjs.dev/guide/testing/marble-testing">test tooling</a> that often came with reactive stream libraries like <a href="https://reactivex.io/">ReactiveX</a>.
It seemed that testing robot applications could benefit from such tooling as well.</p>

<p>Charmed by the initial impressions, I experimented with applying the core concepts from Cycle.js directly to robot application programming.
There were some initial successes.
It delivered on simplifying authoring complex interactive programs and testing such programs.
However, I eventually faced major challenges.
State management in Cycle.js-like frameworks was awkward, especially when it came to composing finite state machines, e.g., requiring techniques that are hard to get right, like <a href="https://speakerdeck.com/p4checo/reactive-state-machines-using-feedback-loops">creating circular dependencies between streams</a>, which was a showstopper because the creating FSMs must be well-supported for the robotics community.
Adopting Cycle.js’ core concepts also didn’t help much with the challenges of building robust applications for distributed robotics systems, such as dealing with unresponsive services, rare process crashes, and subtle performance regressions.
Given such experiences, I felt the strong need for a higher-level abstraction that allows developers to work directly in the application space without worrying about system-level problems[^3].</p>

<figure>
  <img src="https://live.staticflickr.com/3001/2925987725_f9b52f3911_z.jpg" width="480px" />
  <figcaption>Make it so! Photo by <a href="https://www.flickr.com/photos/muffy_larue/">jen.young</a> on <a href="https://www.flickr.com/">flickr</a></figcaption>
</figure>
<p>Looking for new ideas, I explored tools and techniques from the DevOps community, known for their rich experiences working with (large) distributed systems.
The community’s proficiency in employing the declarative approach, i.e., describing the desired state and automating the remaining steps, emphasizing robustness and resilience, was evident in tools (e.g., <a href="https://kubernetes.io/">Kubernetes</a>) and processes (e.g., <a href="https://about.gitlab.com/topics/gitops/">GitOps</a>)[^4].
I liked what I found, but it wasn’t immediately apparent how to apply these findings to the domain of robot application programming.
At the time (2017~2018), DevOps tools were primarily tailored to specific environments (e.g., the cloud) and technologies (e.g., containers) that were foreign to robotics systems.
Nevertheless, as DevOps is a methodology, adopting its practices in the robotics domain has been ongoing[^5].
I hope to catch up and revisit the adoption in a future post.</p>

<figure>
  <img src="https://github.com/vega/vega-lite/blob/main/site/static/teaser.png?raw=true" />
  <figcaption><a href="https://vega.github.io/vega-lite/">Vega-lite</a>, a grammar for rapid data visualization</figcaption>
</figure>

<p>Perhaps the biggest inspiration came from <a href="https://wiki.c2.com/?TheGrammarOfGraphics">the grammar of graphics</a>[^6].
The graphics of grammar offered a concise and structured way to build and explore a large space of data visualization quickly.
There are three core layers of the graphics of grammar:</p>

<ol>
  <li><em>Data</em> is a data frame containing one or more variables.
Fundamentally, data represents categorizable inputs to a visualization system.</li>
  <li><em>Aesthetic</em> defines the mappings of one or more variables to one or more visual elements.
Fundamentally, Aesthetic maps the categorizable inputs to entities in the application space.</li>
  <li><em>Geometry</em> decides the type or shape of the visual elements.
Fundamentally, Geometry gives precise meanings to the mapped entities.</li>
</ol>

<p>We can apply the fundamental structure to the robot application programming domain and build the “grammar of robot applications,” for example:</p>

<ol>
  <li><em>Robot layout</em> defines the physical arrangement of robots and other structures, for example, a layout of a manufacturing line and robot cells.</li>
  <li><em>Task mapping</em> defines how robots and devices map to particular tasks, e.g., insertion, inspection, etc.</li>
  <li><em>Task detailing</em> defines details of the assigned tasks, possibly even the interaction of multiple robots and devices.</li>
</ol>

<p>It’s a rudimentary idea–I don’t know how exactly this grammar will address distributed robotics systems problems like unresponsive services, rare process crashes, and subtle performance regressions.
But one could imagine defining such a grammar or declarative specification that can express the space of particular robot applications, e.g., the space of manufacturing line applications or the space of indoor delivery applications.
Then, the task of developers or solution engineers would be to write a configuration that precisely describes a particular application in mind.
Construction of such a grammar will be nontrivial.
It will require deep understanding and careful organization of the application that will require collaboration with domain experts per application domain.
However, I believe this is the direction that can accelerate the adoption of intelligent robotics services in the target industry.</p>

<p><em>(Update 2023/04/01)</em> There’s been a lot of hype on how ChatGPT–or Large Language Model (LLM)-driven programming synthesis, more generally–will take away programming jobs.
While I don’t fully agree with such hype[^7], I do think ChatGPT will change programming significantly[^8].
What’s interesting to me is that the goal of LLM-driven programming synthesis (LLMSynth) and a grammar-based specification (GSpec) are similar; it is to reduce the huge search space of programming and hence enable developers to concisely describe intended programs.
However, the two take polar opposite approaches; LLMSynth takes the expensive machine learning approach (e.g., requiring lots of data and computing power), and GSpec takes the laborious design approach (e.g., requiring careful design efforts from humans).
As a result, LLMSynth is highly capable (i.e., can do many programming tasks) but not super-precise (i.e., return incorrect programs, sometimes), and GSpec is precise but not as expressive (i.e., can’t do not-designed tasks) but always returns correct programs.
Although I do like to use ChatGPT/Bard/CoPilot, I think using grammar is better for building robot applications because the space of possible programs can be scoped[^9], and the cost of running incorrect problems is extremely high.</p>

<p>What’s also interesting to me is the rise of <a href="https://en.wikipedia.org/wiki/No-code_development_platform">No-code</a>[^10] in the last ~5 years.
Like LLMSynth and GSpec, No-code also shares the goal of simplifying programming and takes a more similar approach to GSpec (than that of LLMSynth), e.g., NoCode tools or visual programming interfaces are designed and built manually (and likely not involve machine learning).
The main difference is the level of expressivity.
Because of their focus on visual programming (targeting non-programmer users), No-Code tools tend to expose a subset of programming interfaces or high-level abstractions to a visual programming interface, resulting in a lower output program space than that of GSpec.
It’s possible for a No-Code tool can expose a high-level grammar, but I haven’t seen tools like that except <a href="https://www.tableau.com/">Tableau</a>.
Btw, No-Code tools have other issues beyond the limited expressivity, e.g., the (different) challenge of supporting non-programmer users and keeping visual programs maintainable.</p>

<h2 id="closing-notes">Closing notes</h2>

<p>I wrote this post to share the idea that I’m excited about–the grammar of robot applications–in its very early form to encourage readers to consider designing a grammar/DSL/declarative specification approach when building their next robot programming tool.
But this post wasn’t just about that.
I shared my observations on the challenges associated with robot application programming and things I’ve tried, e.g., applying tools and techniques from other communities, in the hope of inspiring others to consider taking cross-over approaches–if makes sense.</p>

<p>Let me know what you think by leaving comments below or messaging me on <a href="https://www.linkedin.com/in/michaeljaeyoonchung/">LinkedIn</a> or <a href="https://twitter.com/mjycio">Twitter</a>!</p>

<ul>
  <li>Do my experiences/approaches resonate/align (or not resonate/align) with yours?</li>
  <li>Do you have any robotics (application) problems that would benefit from introducing a DSL?</li>
</ul>

<p>I’d love to hear your thoughts.</p>

<p><br /></p>

<h4 id="footnotes">Footnotes</h4>

<p>[^1] I wrote a <a href="https://mjyc.github.io/assets/pdfs/PhD%20Thesis%20-%20Michael%20Jae-Yoon%20Chung.pdf">thesis</a> on this topic; this post shares my explorations regarding developer tools that didn’t make into my thesis.
<br />[^2] For more insights, see <a href="http://www.gameaipro.com/GameAIPro3/GameAIPro3_Chapter09_Overcoming_Pitfalls_in_Behavior_Tree_Design.pdf">Overcoming Pitfalls in Behavior Tree Design</a>.
<br />[^3] Maybe something like <a href="https://wasp-lang.dev/">Wasp</a> and <a href="https://temporal.io/">Temporal</a> combined.
<br />[^4] For gentle introductions, see <a href="https://youtu.be/VnvRFRk_51k">What is Kubernetes | Kubernetes explained in 15 mins
</a> and <a href="https://youtu.be/f5EpcWp0THw">What is GitOps, How GitOps works and Why it’s so useful
</a>
<br />[^5] <a href="https://github.com/Airbotics/awesome-cloud-robotics">awesome-cloud-robotics</a> provides some ideas.
<br />[^6] For a gentle introduction, see <a href="https://murraylax.org/rtutorials/gog.html#introduction">Introduction to the Grammar of Graphics</a>
<br />[^7] Check out <a href="https://medium.com/bits-and-behavior/large-language-models-will-change-programming-a-little-81445778d957">Large language models will change programming… a little
</a> by Amy Ko.
<br />[^8] Check out <a href="https://medium.com/bits-and-behavior/large-language-models-will-change-programming-a-lot-5cfe13afa46c">Large language models will change programming … a lot</a> by Amy Ko.
<br />[^9] Interestingly <a href="https://twitter.com/WaspLang/status/1678778263735513094">some</a> use LLMSynth and DSL (and not grammar) together and it works?
<br />[^10] For examples, check <a href="https://www.nocode.tech/">NOCODE.TECH</a> and <a href="https://nocodelist.co/">NodeCodeList</a>.</p>]]></content><author><name></name></author><category term="#programming" /><summary type="html"><![CDATA[I've been searching for ways to make robot application programming easy[^1]. Here are my insights on the challenges in programming robot applications and my unfulfilled ideas for (dramatically) simplifying the process.]]></summary></entry><entry><title type="html">Functional Listening and its Generalization: Towards Having Control Over Your Productivity</title><link href="/2021/11/24/functional.html" rel="alternate" type="text/html" title="Functional Listening and its Generalization: Towards Having Control Over Your Productivity" /><published>2021-11-24T08:00:00+00:00</published><updated>2021-11-24T08:00:00+00:00</updated><id>/2021/11/24/functional</id><content type="html" xml:base="/2021/11/24/functional.html"><![CDATA[<figure>
  <img src="https://live.staticflickr.com/7008/6571012801_aaf5cf54f5_k.jpg" />
  <figcaption>84.,   Photo by <a href="https://www.flickr.com/anacarla93/">Ana Carla Machnicki</a> on <a href="https://www.flickr.com/">flickr</a></figcaption>
</figure>

<p>At some point in my life, I realized that I wasn’t listening to music <del>to join the revolution and change the world!</del>to enjoy myself but to put my kids to sleep or to focus while I’m working.
This sad realization made me write this post.</p>

<h2 id="sounds">Sounds</h2>

<p>Like many other new parents, I was introduced to <a href="https://open.spotify.com/playlist/37i9dQZF1DWUZ5bk6qqDSy?si=3494c78a527640a0">white noise</a> sounds when I was struggling to put my son to bed.
One day after a sleepless night, I found myself listening to the white noise sounds even when I was working.
I felt good and focused.</p>

<p>I started exploring more noise sounds to see if any of them can boost my productivity because, at some point, I felt white noise was too harsh for my ears.
Then I found <a href="https://open.spotify.com/playlist/37i9dQZF1DX4hpot8sYudB?si=60a31bdb113441e1">brown noise</a>, which was much warmer and easier on my ears than white noise, and I enjoyed listening to it for awhile.
However, after many days of listening to it for a longer period of time, it became too soothing to the point where it made me feel sleepy, especially on Winter days with long nights.
So I moved on to listening to <a href="https://open.spotify.com/playlist/37i9dQZF1DX5NgkFTxJ4Wv?si=7029f4a7ead24b4a">pink noise</a>, which felt like a great compromise between white noise and brown noise.</p>

<p>Around the same time, my friend, who is also a dad, suggested I check out <a href="https://open.spotify.com/playlist/37i9dQZF1DWUm4vT7WQxcD?si=e76c28f0c6574460">fan noise</a>.
I didn’t initially like it, but it grew on me over time.
There was something about listening to slightly more familiar sounds that kept me listening to it despite the initial dislike.
This observation led me to explore non-synthetic noise sounds like <a href="https://open.spotify.com/playlist/37i9dQZF1DX4PP3DA4J0N8?si=27f759d1d5064e9d">nature sounds</a>, more specifically, <a href="https://open.spotify.com/playlist/37i9dQZF1DWV90ZWj21ygB?si=2db0493068e7492f">ocean sounds</a> and <a href="https://open.spotify.com/playlist/37i9dQZF1DX8ymr6UES7vc?si=f00d14203b384061">rain sounds</a>.
What I really liked about listening to the nature sounds was that not only were they more comfortable to listen to than synthetic noise sounds like white noise, but they also put me into a certain mental state that had a specific effect on me.
For example, listening to the wave noise tricked my brain into putting me into vacation mode, helping me relax, and the heavy rain sounds took me back to the hot and humid monsoon season in South Korea I weirdly enjoyed when I was a kid.</p>

<p>Understanding that listening to certain sounds (or music) can have certain effects on me made me want to leverage such effects.
For example, I intentionally listened to my favorite finals week music from my college years, like <a href="https://open.spotify.com/artist/3Rq3YOF9YG9YfCWD4D56RZ?si=BDrveOO-SRigUDmRDtaeDg">Nujabes</a>-like lo-fi hip-hop music, <a href="https://open.spotify.com/artist/4LEiUm1SRbFMgfqnQTwUbQ?si=mDOsZUMbTQWJe_3lD0DDvw">Bon Iver</a>-like indie folk music, and <a href="https://open.spotify.com/artist/6UUrUCIZtQeOf8tC0WuzRy?si=nm0FY61iTNiNWB3im-vgQA">Sigur Ros</a>-like post-rock music, when I needed to learn new techniques or tools.
When I needed to pump out code, I listened to my favorite coding music from my bachelor years, like <a href="https://open.spotify.com/artist/4tZwfgrHOc3mvqYlEYSvVi?si=AnprvdiGRRKh7DL3-Na_MA">Daft Punk</a>-like French house music or <a href="https://open.spotify.com/artist/1GhPHrq36VKCY3ucVaZCfo?si=HhSfDdriSBKYXEYT5e9zNg">Chemical Brothers</a>-like big beats music.
I turned to nature sounds like ocean sounds to help me calm down and detach from problems, especially when I’m responding to an incident under time pressure.</p>

<h2 id="lights">Lights</h2>

<p>During the pandemic—specifically in the winter of 2020—I learned how sensitive I was to the lighting conditions in my room.
At the time, I was using a single not-so-bright yellow light bulb in my room, and I felt like I couldn’t focus because my room wasn’t bright enough.
So I ordered a <a href="https://a.co/d/8wQJqPb">3-in-1 light socket splitter</a> and three <a href="https://www.wyze.com/products/wyze-bulb-white">smart light bulbs</a> to get my focus back by making my room brighter.</p>

<p>After trying out my new smart light bulbs without any customizations for a few days, I realized that while I really enjoyed having their default bright white lights during the daytime–especially on cloudy days, I didn’t like the same bright white lights in the evening hours.
They didn’t mix well with the other yellow light bulbs we already had.
I felt like the new white lights were taking away the cozy warm feeling created by the existing yellow lights.
I also felt too awake if I worked under bright white lights in the evening and night hours, it was useful if I needed to work long hours, and it made me tired and difficult to concentrate during the day after.</p>

<p>So I used the <a href="https://ifttt.com/">IFTTT</a>-like <a href="https://support.wyze.com/hc/en-us/articles/360032409032-Using-Rules-in-the-Wyze-app">feature</a> in the smart light app to adjust the light bulbs’ brightness and color temperature throughout the day.
Here is a set of rules I created[^1]:</p>

<ul>
  <li><strong>6 am: set brightness to 80%, set color temperature to 50%</strong></li>
  <li>7 am: set brightness to 80%, set color temperature to 50%</li>
  <li><strong>10 am: set brightness to 100%, set color temperature to 100%</strong></li>
  <li>11 am: set brightness to 100%, set color temperature to 100%</li>
  <li>1 pm: set brightness to 100%, set color temperature to 100%</li>
  <li>2 pm: set brightness to 100%, set color temperature to 100%</li>
  <li><strong>4 pm: set brightness to 100%, set color temperature to 75%</strong></li>
  <li>5 pm: set brightness to 100%, set color temperature to 75%</li>
  <li><strong>8 pm: set brightness to 100%, set color temperature to 50%</strong></li>
  <li>9 pm: set brightness to 100%, set color temperature to 50%</li>
  <li><strong>12 am: set brightness to 10%, set color temperature to 1%</strong></li>
</ul>

<p>My smart light bulbs’ brightness ranges from 0 (0%) to 800 lumens (100%), and the color temperature ranges from 2700k (0%) to 6500k (100%)[^2].
There are redundant rules (in the non-bold font) because there was no easy way to <em>keep</em> a certain condition (e.g., set brightness to 100%) <em>throughout</em> a certain time period (e.g., between 10 am and 4 pm) and handle <em>possible failures</em> due to <em>unreliable connections</em>[^3] using the IFTTT-like feature that came with the bulb[^4].
For example, if I power off my lights at 10 pm and power them back on at 9am the next day, the brightness and color temperature will stay at what they were at 10 pm last night and they won’t change until the next rule kicks in, e.g., at 10 am.
One of the ways to get around this issue was using the “<a href="https://www.digitaltrends.com/home/wyze-bulb-white-sun-match-mode-changes-based-on-sun/">sun match</a>” feature that continuously adjusts the brightness and color temperatures to match that of the sun throughout the day.
However, I liked having a set of rules that changes brightness and color temperature in a bit more discrete manner.</p>

<p>A funny thing about having these rules was that they had a reminder-like effect on me.
For example, adjusting the color temperature slightly at 4 pm was like a reminder that I should wrap up and get ready for picking up my son at 5:30 pm, whereas the 5 pm change was like a “final warning.” 
The dramatic reduction of brightness at 12 am was like a reminder that I should go to bed even though there is this one thing I really want to finish.
It was really effective, especially because I like to hide the menubar, where the clock is located, to maximize screen space and minimize distractions when I’m working.</p>

<h3 id="screen-color-temperature-dynamic-wallpaper">Screen Color Temperature, Dynamic Wallpaper</h3>

<p>Once I understood my room lights’ reminder-like effect on me, I looked for other things that I can control, e.g., to better control myself.
The first one was the screen display color temperature.</p>

<p>I’m not a fan of dark mode for my workstation desktop.
I know I’m a minority among fellow developers, but I just find using a white background (e.g., on IDE and terminal) easier to read.
However, one problem with using the white background is it hurts my eyes towards the end of the day, i.e., in the evening hours.
I became a big fan of the “night <a href="https://support.apple.com/en-us/HT207513">shift</a>/<a href="https://help.ubuntu.com/stable/ubuntu-help/display-night-light.html.en">light</a>” feature that adjusts the screen display color to a warmer color after sunset because it made using a white background in the evening much more comfortable.</p>

<p>To go beyond the default “start night shift at sunset and stop it at sunrise”, I set up the following cron jobs on my work desktop running Ubuntu:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Run `sudo crontab -e` to open this file</span>

0 0 <span class="k">*</span> <span class="k">*</span> <span class="k">*</span> /usr/bin/gsettings <span class="nb">set </span>org.gnome.settings-daemon.plugins.color night-light-temperature 1000
0 6 <span class="k">*</span> <span class="k">*</span> <span class="k">*</span> /usr/bin/gsettings <span class="nb">set </span>org.gnome.settings-daemon.plugins.color night-light-temperature 5000
0 10 <span class="k">*</span> <span class="k">*</span> <span class="k">*</span> /usr/bin/gsettings <span class="nb">set </span>org.gnome.settings-daemon.plugins.color night-light-temperature 6000
0 4 <span class="k">*</span> <span class="k">*</span> <span class="k">*</span> /usr/bin/gsettings <span class="nb">set </span>org.gnome.settings-daemon.plugins.color night-light-temperature 5000
0 5 <span class="k">*</span> <span class="k">*</span> <span class="k">*</span> /usr/bin/gsettings <span class="nb">set </span>org.gnome.settings-daemon.plugins.color night-light-temperature 4000
0 8 <span class="k">*</span> <span class="k">*</span> <span class="k">*</span> /usr/bin/gsettings <span class="nb">set </span>org.gnome.settings-daemon.plugins.color night-light-temperature 3000
</code></pre></div></div>
<p>By the way, here I didn’t need to have redundant rules because my laptop was always on.
I even applied the same idea to control <a href="https://github.com/adi1090x/dynamic-wallpaper">dynamic wallpaper</a>, noticing dynamic wallpaper is another thing that I implicitly interact with.</p>

<p>For some time, I played with synchronizing such changes across the room lights, screen display, and wallpaper but screen color temperature and wallpaper rules didn’t stick.
I ended up just using the default nightshift mode (i.e., on/off on sunset/sunrise) because changing the screen display color temperature was too subtle to give me the kick.
As for the dynamic background, I often missed the changing background because I usually maximize my IDE or terminal window.</p>

<h2 id="productivity">Productivity</h2>

<p>My exploration of trying to understand the effects of various kinds of lights on myself reminded me of human-computer interaction (HCI) research papers investigating the possibility of behavior change via technologies (disclosure: I only read their abstracts):</p>

<ul>
  <li><a href="https://www.researchgate.net/publication/221515000_The_design_of_eco-feedback_technology">The design of eco-feedback technology</a></li>
  <li><a href="https://www.researchgate.net/publication/221514889_Theory-driven_design_strategies_for_technologies_that_support_behavior_change_in_everyday_life">Theory-Driven Design Strategies for Technologies that Support Behavior Change in Everyday Life</a></li>
  <li><a href="https://www.researchgate.net/profile/Julie-Kientz/publication/220962690_Personality_and_Persuasive_Technology_An_Exploratory_Study_on_Health-Promoting_Mobile_Applications/links/0deec5191b0d334f93000000/Personality-and-Persuasive-Technology-An-Exploratory-Study-on-Health-Promoting-Mobile-Applications.pdf">Personality and Persuasive Technology: An Exploratory Study on Health-Promoting Mobile Applications</a></li>
  <li><a href="http://cs4760.csl.mtu.edu/2022/resources/HE2.pdf">Heuristic evaluation of ambient displays</a></li>
</ul>

<p>Then I thought, can we take a computational approach to figure out the optimal environment (e.g., sounds/lights sequence/placements) for productivity?
For example, if we have an accurate model of how certain environments like sounds or lights affect our productivity, we could find an optimal sequence and placement of sounds and lights that maximizes our productivity!
Building such an accurate model would be difficult, but maybe we can take a <a href="https://quantifiedself.com/">quantified-self</a> approach to collect data and augment it with data from smart home devices?</p>

<p>But then I asked myself: Do I really want to go this route? Would it truly boost my productivity?</p>

<h3 id="self-control-via-environment-control">Self-Control via Environment Control</h3>

<p>The goal of this entire journey was to improve my productivity, and what I discovered was not just about coffee but rather about self-control.
This realization stemmed from the following observations:</p>

<ul>
  <li>Certain things put me into certain moods (“Sound”).</li>
  <li>Environmental cues act as triggers (“Lights”).</li>
</ul>

<p>And thinking about how to leverage these observations.
Expanding to different modalities (“Screen Color Temperature, Dynamic Wallpaper”) taught me how to avoid distractions and stay focused on my goals.
One thing I’ve learned from daydreaming about the computational approach to boosting my productivity (“Idea”) is that, as a first step, I could try to internally learn my productivity model by paying more attention to how my mind responds to various environments (let’s meditate people!).
Only then can I leverage such a model (understanding) by changing environments.</p>

<p><br /></p>

<h4 id="footnotes">Footnotes</h4>

<p>[^1] And <a href="https://gist.github.com/mjyc/ad2de6e348f8be86e335be53cfb38fe4#file-office-room-lights-yaml">here</a> is a similar set of rules in <a href="https://developers.home.google.com/automations/schema?hl=en">Google Home Automation Script</a>.
<br />[^2] For more information on the color temperature unit Kelvin (K), see <a href="https://www.ledlightexpert.com/understanding_led_light_color_temperatures_ep_79">this article</a>.
<br />[^3] Especially the case if you have many (e.g., &gt; 10) devices. You more likely to likely to have some (e.g., Wi-Fi connection, etc.) issues. There are some <a href="https://www.youtube.com/@HomeAutomationGuy">people</a> on internet who have done some serious home-automation projects. I’d checkout their contents for insights.
<br />[^4] I heard some products better supprot “state-based”–as opposed to “event/trigger-based”–actions. Btw, my gradschool labmate published a <a href="https://hcrlab.cs.washington.edu/assets/pdfs/2015/huang2015ubicomp.pdf">research paper</a> about a related challenge.</p>]]></content><author><name></name></author><category term="#productivity" /><summary type="html"><![CDATA[84., Photo by Ana Carla Machnicki on flickr]]></summary></entry><entry><title type="html">Testing robotics systems in fast-paced startups</title><link href="/2020/12/16/testing.html" rel="alternate" type="text/html" title="Testing robotics systems in fast-paced startups" /><published>2020-12-16T08:00:00+00:00</published><updated>2020-12-16T08:00:00+00:00</updated><id>/2020/12/16/testing</id><content type="html" xml:base="/2020/12/16/testing.html"><![CDATA[<figure>
  <img src="https://live.staticflickr.com/195/506281600_a68f821d33_c.jpg" />
  <figcaption>Starcraft II, Photo by <a href="https://www.flickr.com/photos/tirrell/">Zach Tirrell</a> on <a href="https://www.flickr.com/">flickr</a></figcaption>
</figure>

<p>Testing robotics systems is hard.
Based on my experience working at startups with fewer than 200 employees and fewer than 100 robots providing RaaS using fleets of indoor mobile robots or lines of robot manipulators, the main reasons for the difficulty were as follows:</p>

<ul>
  <li>Edge cases and corner cases in production environments.</li>
  <li>The difficulty of using simulation.</li>
  <li>Challenges with adopting automation.</li>
</ul>

<h2 id="why-is-testing-robotics-systems-hard">Why is Testing Robotics Systems Hard?</h2>

<p>In <strong>production</strong>, I’ve encountered various <strong>edge cases and corner cases</strong>:</p>

<ul>
  <li><em>Edge cases for robotics algorithms.</em>
  Input spaces for robotics algorithms, such as perception, control, and motion planning, are vast and challenging to effectively cover for edge cases; there is always a specific layout that causes navigation failures or a particular scene with specific objects that leads to grasping failures.
  Characterizing such instances is difficult and algorithm-dependent, which complicates the testing setup.</li>
  <li><em>Rare hardware issues.</em>
  Rare hardware issues that are not (directly) detectable are the worst, such as a small damage in the robot cell structure that requires adjusting the collision map.
  Anticipating such issues requires input from domain experts (e.g., mechanical or firmware engineers), who may not be easily accessible and speak different jargons and reproducing them often requires changing interfaces, which can be expensive (e.g., it becomes yet another layer to maintain).</li>
  <li><em>Subtle regression.</em>
  The complexity of robotics systems makes it challenging to establish a robust <a href="https://katalon.com/resources-center/blog/regression-testing">regression testing</a> pipeline.
  For example, handling low-frequency <a href="https://docs.gitlab.com/ee/development/testing_guide/flaky_tests.html">flaky tests</a>, implementing robust test selection and prioritization[^2] is difficult and hence elusive bugs slip back into the production code.
  Performance regressions are particularly challenging–especially ones caused by low-level concurrency issues[^3], as they are subtle and require expensive measures such as repeated end-to-end tests and delicate statistical methods to detect.</li>
  <li><em>Unexpected peak load condition/usage pattern.</em>
  It is common for multiple (custom) software, such as core robotics, monitoring, and infra-related software, to run in parallel.
  Unexpected high demands can adversarially impact your program, e.g., by consuming all of the available resources.
  Anticipating and recreating such situations is challenging, especially when dealing with (custom) software at all levels, including firmware and system software.</li>
</ul>

<p><strong>Using simulation</strong> for testing robotics systems effectively <strong>is not as easy</strong> as it seems.</p>

<ul>
  <li><em>Inadequate usage.</em>
  I find that simulation testing is most useful for end-to-end testing of robot applications.
  However, I often encounter test cases that would benefit from using other tools and techniques (e.g., for efficiency).
  All too frequently, I come across test cases for robot behaviors (e.g., implemented in finite state machine or behavior tree) that use simulation, making the test cases much more expensive than they need to be.
  In such cases, using alternatives like <a href="https://martinfowler.com/bliki/TestDouble.html">fake</a> or <a href="https://www.educative.io/answers/what-is-model-based-testing">model-based testing</a>[^4] would be much more efficient, as they can be used to only “simulate” the directly relevant modules[^5].
  This often stems from organizational issues, such as unclear boundaries between teams that result in poorly defined interfaces and testing strategies[^6] or more typical insufficient allocation of time for testing/addressing technical debts (e.g., in favor of prioritizing other deliverables).</li>
  <li><em>Generating test cases.</em>
  Even with simulation libraries that provide high-level interfaces for building scenarios, creating effective test scenarios is challenging.
  Creating a single simulated environment for end-to-end testing alone is laborious enough, so diversifying the test scenarios (e.g., to cover extreme cases) becomes a nice-to-have[^7].
  There are commercial products that address this issue (e.g., <a href="https://aws.amazon.com/blogs/aws/aws-announces-worldforge-in-aws-robomaker/">AWS RoboMaker WorldForge</a>), but they are not easy for smaller organizations (i.e., startups) to integrate due to reasons such as integration cost, vendor lock-in, etc.</li>
  <li><em>Expressing specifications.</em>
  Specifications for many robotics programs, e.g., those involving perception, motion planning, and behaviors, are difficult to express due to their spatiotemporal nature.
  This leads to verbose and unorganized (e.g., containing duplicates) test code, which makes it difficult to maintain and scale.</li>
  <li><em>Managing infrastructure.</em>
  I haven’t met a single person who loves managing simulation testing infrastructure, e.g., for continuous integration.
  Simulation test code is expensive to run, requires special hardware such as GPUs, and is difficult to optimize and move around (e.g., in cloud environments).
  This leads to a poor developer experience and can even result in the disabling of simulation testing.</li>
</ul>

<p><strong>Automating</strong> robotics software <strong>testing is still hard</strong>.</p>

<ul>
  <li><em>Challenges with automating build and deployment.</em>
  Here are tech talks and a blog post that shed light on this topic:
    <ul>
      <li><a href="https://picknik.ai/ros/debian/packaging/2023/02/27/packaging-ros-with-github-actions.html">Packaging ROS with GitHub Actions</a> from PICKNIK Blog, 2023.</li>
      <li><a href="https://youtu.be/fjfFe98LTm8">Building Self Driving Cars with Bazel</a> from Cruise, BazelCon 2019 - shares Cruise’s experiences with building and testing robotics software at scale</li>
      <li><a href="https://web.archive.org/web/20230330175041/https://www.airbotics.io/blog/software-deployment-landscape">The landscape of software deployment in robotics</a> from Airbotics - summarizes the typical challenges with deploying robotics software</li>
      <li><a href="https://youtu.be/JNV9CkARh_g">Physical continuous integration on real robots</a> from Fetch, ROSCon 2016 - shares Fetch’s experience with setting up and using a physical continuous integration pipeline</li>
    </ul>
  </li>
  <li><em>No standard.</em>
  Automating the testing of software requires agreements among engineering teams on build, deployment, and test models.
  Given how robotics brings multiple communities together, such as research (e.g., computer vision, robotics), web development (e.g., frontend, backend), DevOps, embedded, etc., reaching such an agreement, or even discussing ideas (e.g., due to different backgrounds), is difficult.
  While the Robot-Operating System (ROS) and the communities around it have made significant progress in this regard, the lack of standards still seems to be a significant problem in organizations.</li>
</ul>

<h2 id="where-to-start-with-testing-general-techniques">Where to Start with Testing: General Techniques</h2>

<p>To figure out where to start with testing a robotics system, I use the followings:</p>

<ul>
  <li>Prioritization framework for creating tests.</li>
  <li>Systematic procedure for identifying what to test.</li>
</ul>

<p>The techniques discussed in this section are general/not-so-technical and are mostly aimed at addressing the abovementioned challenges regarding “edge and corner cases in production environments”.</p>

<h3 id="eisenhower-matrix-for-test-prioritization">Eisenhower Matrix for Test Prioritization</h3>

<p>In robotics startups that build complex systems, creating comprehensive test suites is impossible.
To produce high-impact tests within the time budget, I use my adapted <a href="https://www.eisenhower.me/eisenhower-matrix/">Eisenhower Matrix</a> to prioritize a list of failure scenarios by first categorizing (potential) failure scenarios according to their (expected) frequency and risk.</p>

<figure>
  <img src="/assets/imgs/mcmatrix.png" />
</figure>

<ul>
  <li><strong>First Quadrant (upper left): frequent and high-risk.</strong>
  In Quadrant 1 (Q1), I place failure scenarios that need to be covered immediately, e.g., that I hear all the time from internal communication channels, such as introducing breaking changes to APIs and dependencies.</li>
  <li><strong>Second Quadrant (upper right): frequent and medium-risk.</strong>
  In Quadrant 2 (Q2), I place failure scenarios that occur frequently but allow continued operations with short downtime like unreliable hardware or unresponsive user interface issues with well-established alerts and recovery procedures.</li>
  <li><strong>Third Quadrant (lower left): infrequent and high-risk.</strong>
  In Quadrant 3 (Q3), I place failure scenarios that occur rarely but causes significant disruption in operations such as core robotics component failure scenario or unexpected peak usage pattern.</li>
  <li><strong>Fourth Quadrant (lower right): infrequent and medium-risk.</strong>
  In Quadrant 4 (Q4), I place failure scenarios that occur relatively infrequently and allow continued operations.</li>
</ul>

<p>The placement of example failure scenarios in quadrants will differ across companies.
For instance, depending on the maturity of the robot product/prototype or the amount of time invested by the engineering team in designing the system, an unreliable hardware failure scenario may belong in Q1 (e.g., if it is causing multiple issues) or a robotics algorithms failure scenario may belong in Q2 (e.g., if the failure is not catastrophic or easily recoverable).
In general, I create or improve tests for one quadrant at a time, in increasing order.
After working on tests for Q1, I move on to tests for Q2 before addressing those for Q3.
This is because creating tests for Q3 requires a significant time investment, for example, to ensure the reproducibility of the failure.
Usually, there is no time available to work on tests for Q4.
But adjustments should be made to meet organization-specific requirements and constraints.</p>

<h3 id="test-scenario-identification-procedure">Test Scenario Identification Procedure</h3>

<p>So far, I have assumed that the failure scenarios to test are known; however, this is usually not the case.
To determine what to test, I follow these steps:</p>

<ol>
  <li><strong>Gain access to internal alerts, dashboards, and logs.</strong>
 Investigating recently reported problems or analyzing the latest trends using monitoring tools[^8] is the easiest way to identify high-risk failure scenarios.
 If monitoring tools are not set up (e.g., in smaller companies), I get involved in operations work, which is another way to uncover potential high-value tests to create.</li>
  <li><strong>Identify interface and service boundaries.</strong>
 Understanding how software components interact with each other provides insights into potential integration failures and their impact.
 I start by looking for internal documentation with system diagrams (or examining the codebase and creating them if such diagrams don’t exist) and ask questions such as: which interactions must not fail? which interactions are changing frequently?
 Such exercises reveal missing must-have contract tests or high-impact opportunities to improve integration tests.</li>
  <li><strong>Identify implicit dependencies.</strong>
 I consider edge cases such as low resources, unexpected hardware states, or unseen inputs to robotics algorithms (e.g., those that crash applications) as unmet runtime dependencies.
 Taking this view nudges me to specify these not-well-understood requirements for keeping the system (or “implicit dependencies”) well-behaving as explicitly and clearly as possible.
 Once defined, such requirements can be used to create extreme failure scenarios to test.</li>
</ol>

<h2 id="ensuring-testability-as-a-startup-grows">Ensuring Testability as a Startup Grows</h2>

<p>Below, I share my insights on key practices to employ at each growth stage/funding round of robotics startups.
The real motivation behind testing is the reliability (e.g., of the provided service), and so the shared key practices below cover related areas such as debugging and observability[^9].</p>

<p>If you have experience with SaaS/web service startups, you might notice that the RaaS/robotics company size (i.e., the number of employees) at each growth stage is larger, and some key practices occur later in robotics startups.
This is because RaaS/robotics products not only consist of a more diverse set of software components but also require additional teams like electrical/firmware engineering, mechanical engineering, and manufacturing operations.</p>

<h3 id="5-20-employees">5-20 Employees</h3>

<p>At this stage, startups are likely to have fewer than 5 customers/design partners and a handful of developers who are relentlessly building (and fixing) major components of the company’s first product.
The goal of the startups is to prove the value of their product to their (rather forgiving) customers by succeeding in basic tasks performed by robots as much as possible.
For example, a delivery robot should navigate without colliding with obstacles, and a robot manipulator should pick and place objects without dropping them in customers’ (production) environments.</p>

<p><strong>Key Practices:</strong></p>

<ul>
  <li>Setting up <strong>continuous integration or nightly tests</strong> (e.g., using Jenkins, GitLab CI/CD, GitHub Actions, or <a href="https://picknik.ai/ros/debian/packaging/2023/02/27/packaging-ros-with-github-actions.html">Debian build farm</a>) with <strong>end-to-end tests involving a high-fidelity simulator</strong> (e.g., Gazebo) to quickly smoke test rapidly changing codebases.</li>
  <li>Collaborating on <strong>internal communication channels</strong> (e.g., Slack) and utilizing a (custom) <strong>teleop solution</strong> (e.g., built on MQTT, WebRTC) to quickly respond to critical incidents.</li>
  <li>Creating <strong>metrics and dashboards to track business-critical measures</strong> (e.g., using Grafana/Prometheus) such as the number of deliveries/distance traveled, throughput/object knitted to guide all developers/employees at a high level.</li>
</ul>

<h3 id="21-200-employees">21-200 Employees</h3>

<p>Startups at this stage start expanding their customer base and aim to scale their operations to deploy and handle, for example, more than 100 robots.
The companies now have (small) teams of developers working on enhancing the robustness of core robotics software components to handle diverse environments of new (and unforgiving) customers, providing a non-beta-user-acceptable user experience (e.g., by building proper onboard and/or desktop UIs), and/or infrastructure to scale operations.
The robotics system powering the product becomes much more complex, and to manage such complexity, its architecture becomes more modular, composable, and distributed, and boundaries/ownerships occur.</p>

<p><strong>Key Practices:</strong></p>

<ul>
  <li>Employing <strong><a href="https://martinfowler.com/bliki/IntegrationTest.html">narrow integration tests and contract tests</a></strong> using <a href="https://martinfowler.com/bliki/TestDouble.html">test doubles</a> like fakes and spies—sometimes going as far as implementing low-fidelity simulators with them (e.g., using pytest, GoogleTest) to test each team’s evolving software component in isolation and complex interactions between such components, efficiently.</li>
  <li>Establishing a <strong>deployment strategy with rollback support</strong> (e.g., using tools that enable infrastructure-as-code/gitops like Ansible, Terraform and support over-the-air updates, etc.) to avoid manually applying untrackable hotfixes in fear of losing customers. <br />
  By the way, this practice is nontrivial to achieve for technical and cultural reasons; for more information, see <a href="https://www.airbotics.io/blog/software-deployment-landscape">“The landscape of software deployment in robotics”</a> from Airbotics.
  Typically, adopting a deployment tooling sparks discussions like adopting cloud-native tooling <a href="https://ubuntu.com/blog/ros-docker">or not</a>, which then sparks discussions on <a href="https://discourse.ros.org/t/how-do-you-launch-your-systems/23383/16">system launching mechanisms</a>, and so on.</li>
  <li>Implementing <strong>nightly tests on real robots</strong> in a mirror warehouse or manufacturing line or <strong><a href="https://youtu.be/JNV9CkARh**g">physical continuous integration</a></strong> in a production-like environment to prevent edge and corner cases and performance regressions in production environments as much as possible.</li>
  <li><strong>Structuring and centralizing logs</strong> (e.g., structlog, spdlog, and Loki, ELK, Splunk) to track performance across growing fleets and enable dynamically querying information for fast debugging or monitoring, e.g., with <a href="/2023/04/21/observability.html">robotics-specific metrics</a>.
  In addition, centralizing data would be ideal; however, given the large size of robotics data, doing so requires more care and resources—which usually becomes possible to afford in the next stage.</li>
  <li>Implementing a <strong>data record and replayer and visualizer</strong> (e.g., rosbag and RVIZ, Foxglove Studio, or custom-built ones) to enable debugging or optimizing robotics algorithms running in distributed systems.
  By the way, building a performant data record and replayer, e.g., that works well with both real robots and simulators, is no joke; there are issues like not being able to record or replay data fast enough, doesn’t work well with simulators that run slower or faster than real-time, etc.</li>
  <li>Utilizing (custom) <strong>fleet operation solution</strong> to scale teleop-based incidence recovery (e.g., more robots to monitor/rescue per person), open up such operations to non-developers, etc.</li>
</ul>

<h3 id="201-2000-employees">201-2000 Employees</h3>

<p>I have not been employed by a robotics company that had more than 200 employees.
The insights in this subsection are derived from (1) my experience at a company was actively preparing for the expansion beyond 200 employees and (2) indirect experiences shared by my colleagues in the robotics field.
In other words, please take the suggested key practices with a grain of salt.</p>

<p>Robotics companies at this stage have experienced significant growth and have a large customer base.
Their goal is to further scale their operations and efficiently handle a substantial number of robots, potentially exceeding 1000 units, without compromising performance.
The company now consists of multiple teams working on various aspects of the robotics system, including core software development, fleet management and user experience enhancement, custom hardware integration and test, infrastructure scalability, and customer support.
Additionally, the focus expands beyond purely technical challenges to encompass broader organizational considerations, such as team structure, talent management, strategic partnerships, and market expansion.</p>

<p><strong>Key Practices:</strong></p>

<ul>
  <li>Implementing an <strong>advanced test automation framework</strong> that encompasses a wide range of tests, including unit, integration, performance regression (e.g., via hardware-in-the-loop), and security tests.
  To improve testing efficiency, the company utilizes techniques such as <a href="https://martinfowler.com/articles/rise-test-impact-analysis.html">test impact analysis</a> and test selection and prioritization, which identify the most critical tests and prioritize their execution.
  Furthermore, they leverage cloud resources to run a large number of simulation-based end-to-end tests in parallel, accelerating the testing process.</li>
  <li>Enhancing <strong>developer productivity</strong> through faster iteration, improved debugging and testing tooling, and ultimately aiming to enhance system reliability.
  At this stage, companies have the resources to invest in infrastructure and pipeline improvements.
  This includes setting up <strong>cloud development environments</strong> utilizing platforms like GitPod, Coder, and DevZero.
  Additionally, efforts are made to <strong>speed up build times</strong> through techniques such as <a href="https://bazel.build/remote/caching">remote caching</a>.
  <strong>Streamlining code review and merge progress</strong> is achieved using tools like Merge Queue.
  Furthermore, <strong>specialized tooling</strong> is developed to cater to the specific needs of the company.
  For instance, enhancing the data record and replayer to support <strong>streaming robotics data</strong> enables faster debugging by bypassing the need to download large datasets onto local development machines.
  The data record and replayer are also optimized to handle high throughput and enable navigation between critical timestamps.
  Another example of specialized tooling is the creation of a <strong>test scenario generator</strong>, which automates the generation of realistic test scenarios, enhancing test coverage and enabling the identification of edge cases and potential failure scenarios.</li>
  <li>Establishing a <strong>formalized release management</strong> process that includes thorough testing, staging environments, and controlled deployments across all teams involved in building a product.
  This involves standarizing continuous integration and continuous deployment (CI/CD) practices across teams, maintaining release documentation, and employing release dashboards.
  The company also invests in <strong>advanced deployment and orchestration</strong> tools such as Terraform, Docker, and Kubernetes to facilitate large-scale, cross-team deployment and operation of a diverse fleet, encompassing robots, peripheral devices (e.g., elevator controller, manufacturing line controller), and (web) servers.</li>
  <li>Implementing <strong>incident response and resolution workflows</strong> to minimize downtime and efficiently handle critical incidents.
  The fleet operation tool should leverage automation, e.g., incident detection, require minimal human input for recovery, etc.
  Also the workflow or procedure should allow each team to be responsible for the sfotware they write.</li>
  <li>Establishing a <strong>security and compliance framework</strong> to ensure the protection of sensitive data, intellectual property, and customer privacy.
  This includes implementing secure coding practices, conducting regular security audits, and complying with relevant industry standards and regulations.</li>
</ul>

<h3 id="beyond-2000-employees">Beyond 2000 Employees</h3>

<p>I don’t have any experience with companies at this size.
In fact, I am not even sure if any of robotics companies that I know is at this stage–maybe some of the large autonomous vehicle companies like Waymo, Cruise, Aurora, and Zoox?
I will update here after gaining more experience with companies at this stage, one day.</p>

<h2 id="closing-notes">Closing notes</h2>

<p>In this post, I have listed the challenges of testing robotics systems in fast-paced startups, shared my techniques for getting started with testing work, and provided insights on key practices for ensuring testability as an organization grows.</p>

<p>Let me know what you think by leaving comments below or messaging me on <a href="https://www.linkedin.com/in/michaeljaeyoonchung/">LinkedIn</a> or <a href="https://twitter.com/mjycio">Twitter</a>!</p>

<ul>
  <li>Do my experiences/approaches resonate/align (or not resonate/align) with yours?</li>
  <li>Do you have any test-related war stories or effective testing strategies you’d like to share?</li>
</ul>

<p>I’d love to hear your thoughts.</p>

<p><br /></p>

<h4 id="acknowledgements">Acknowledgements</h4>

<p>I thank Rastislav Komara, Dhiraj Goel, Michael J. Declerck, and Doug Blinn for sharing their experiences and insights.
I also thank Jihoon Lee, Christian Fritz, and Jimmy Baraglia for their feedback on the initial draft.</p>

<h4 id="significant-revisions">Significant Revisions</h4>

<ul>
  <li><em>2023/06/29</em>: Added the “Ensuring reliability as an organization grows” section</li>
  <li><em>2023/05/28</em>: Rewrote the whole post</li>
</ul>

<h4 id="footnotes">Footnotes</h4>

<p>[^1] The identified challenges and my approaches may not generalize to other settings, such as testing in robotics companies that are much smaller (i.e., &lt; 10 employees) or much bigger (i.e., &gt; 1000 employees), or involving a different product, such as an autonomous vehicle-based ride-hailing service or an autonomous-based inspection service.
For example, I don’t have much experience with testing robotics systems that make heavy use of <a href="https://getcruise.com/news/blog/2020/cruises-continuous-learning-machine-predicts-the-unpredictable-on-san/">machine learning</a> or <a href="https://docs.ros.org/en/iron/index.html">real-time programming</a>.
<br />[^2] See also <a href="https://martinfowler.com/articles/rise-test-impact-analysis.html">The Rise of Test Impact Analysis</a> by Martin Fowler.
<br />[^3] E.g., see <a href="https://youtu.be/II8yCw5tPE0">ROSCon 2017 Vancouver Day 2 Determinism in ROS</a> and <a href="https://vimeopro.com/osrfoundation/roscon-2019/video/379127709">ROSCON 2019 MACAU: CONCURRENCY IN ROS 1 AND 2: FROM ASYNCSPINNER TO MULTITHREADEDEXECUTOR</a>.
<br />[^4] For code examples, see <a href="https://hypothesis.readthedocs.io/en/latest/stateful.html">Stateful testing</a> (Python) or <a href="https://medium.com/criteo-engineering/detecting-the-unexpected-in-web-ui-fuzzing-1f3822c8a3a5">Detecting the unexpected in (Web) UI</a> (JavaScript).
<br />[^5] Check out <a href="https://martinfowler.com/bliki/IntegrationTest.html">IntegrationTest</a> by Martin Fowler for related discussion, e.g., about narrow and broad integration tests.
<br />[^6] Check out <a href="https://martinfowler.com/articles/2021-test-shapes.html">On the Diverse And Fantastical Shapes of Testing</a> (at least the last paragarph starting with “If you’re paying my careful prose …”) by Martin Fowler for related discussion.
<br />[^7] I enjoy following research papers in this space, such as those taking a grammar-based approach like <a href="https://dl.acm.org/doi/abs/10.1145/3314221.3314633">Scenic: a language for scenario specification and scene generation</a>.
<br />[^8] See also <a href="https://twitter.com/GergelyOrosz/status/1665340939529773057">this twit thread</a> from Gergely Orosz.
<br />[^9] See also my another post <a href="/2023/04/21/observability.html">Robo-Observability</a>.</p>]]></content><author><name></name></author><category term="#softwareengineering" /><summary type="html"><![CDATA[Testing robotics systems is hard. Based on my experience working at startups with fewer than 200 employees and fewer than 100 robots providing RaaS using fleets of indoor mobile robots or lines of robot manipulators, the main reasons for the difficulty were as follows:]]></summary></entry><entry><title type="html">Finishing Graduate School as a New Dad or: How I Learned to Stop Worrying and be Efficient</title><link href="/2020/11/30/finishing.html" rel="alternate" type="text/html" title="Finishing Graduate School as a New Dad or: How I Learned to Stop Worrying and be Efficient" /><published>2020-11-30T08:00:00+00:00</published><updated>2020-11-30T08:00:00+00:00</updated><id>/2020/11/30/finishing</id><content type="html" xml:base="/2020/11/30/finishing.html"><![CDATA[<p>I was a graduate student when my son was born.
As much as I was euphoric about my son’s arrival, reality hit me hard.
I was not ready.
It was brutal but I pulled through graduate school and here is s selected list of tricks I’ve learned from having been a graduate student parent.</p>

<h2 id="buying-time">Buying time</h2>

<p>Time became one of the scarcest resources.
The first thing I did to buy more time for research was saying “no” to social activities, meetings with no clear goal, not-so-related review requests, side projects, system upgrades, etc. as most parents do.
I decided what activities/projects/commitments to say no to based on my gut feeling.
One big problem with this approach was that I couldn’t tell if I missed interesting and important opportunities that are unforeseeable from the day I said “no”.
I started to explicitly allocate time for explorations and update my goals/plans frequently to adjust my plans based on findings from the explorations[^1].
Doing this became easier as I became more aware of my own work/research velocity, which I gained slowly by rigorously tracking my time usage and goals for each week/month/year.</p>

<p>I still felt like there wasn’t enough time for work hence the second thing I tried was securing money, e.g., by asking family members and applying for financial aids available via my department or university.
I did not know about any financial support opportunities but learned about them as time goes on by talking to a few other graduate student parents that I didn’t know the existence of prior to becoming a dad.</p>

<p>The final trick I learned was delaying tasks[^2].
Delaying tasks is a great trick because often delayed tasks disappear completely due to priority changes.
In the beginning, I delayed tasks that I felt very comfortable delaying, e.g., polishing plots before submitting a paper or renaming variable names in codebase while setting up experiments.
Gradually, I started delaying seemingly more important tasks such as polishing related work sections or optimizing infrastructure/refactoring codebases (for initial submissions) essentially to verify get feedback on the more important content earlier, e.g., the main research direction.
In practice, the most difficult part was knowing what kind of tasks are okay (or not okay) to delay on at first sight; sometimes delaying seemingly not-so-important tasks comes back after becoming a much bigger task.
To mitigate this risk, I started to put some time to identify the worst consequences of delaying a certain task and prepare <em>a</em> plan for the worst outcomes.</p>

<h2 id="minimal-viable-product">Minimal viable product</h2>

<p>After trying to buy as much time as possible for about a year or so, I made the following observations:</p>

<ol>
  <li>achieving the last 10~20% takes as much more effort as achieving the first 80~90%</li>
  <li>re-visiting/working on something always takes much more time than the initial take</li>
  <li>there is a huge difference between having something finished vs. not, e.g., at least you get a chance to receive feedback</li>
</ol>

<p>These observations made me want to always shoot for the minimal viable product (MVP).
In the past, I tended to overshoot because I didn’t understand the evaluation criteria well and hence wanted to be “safe” by achieving an arbitrary high quality, which was an extremely expensive approach because of 1. and 3. (sometimes I gave up when a task became too big).
To address this problem, I spent more time on (1) understanding the evaluation metrics well, and (2) prototyping early to feel out the requirements in the context.
After clearly defining the evaluation metrics for the MVP, I found using them to efficiently execute something to the completion super helpful, esp., towards the end when I’m tired.
<!-- Another small trick I used was holding my breath and split once without looking back. --></p>

<h2 id="managing-attention">Managing attention</h2>

<p>After having countless sleepless nights and physically demanding days, I’ve realized the most expensive resource was my attention and not the time.
The quality of my attention was not only dependent on my physiological conditions but also my surrounding environment, e.g., the amount of natural light, my team’s mood, etc.
With everything going on, I usually only had about 2hrs of the peak attention per day; 4hrs if lucky.</p>

<p>To best use this time, I identified the most likely time of the day that I have the best attention and protect that time slot for the most important tasks.
Whenever I face a big task, I would start breaking it down and think about different approaches while I’m away from the desk, e.g., while commuting, picking up or dropping off my son, etc.
Usually, the most important tasks such as core algorithm/system design, initial draft writing, big meeting/presentation, etc. reveal themselves[^3].
These tasks require deeply exploring many ideas with caution to make a big dent towards a knotty problem[^4], so I used my protected time to only work on these tasks.
I used times with a medium level of attention for execution tasks; I used to do execution tasks in the protected hours but with well-thought-out approaches, execution tasks didn’t require as much attention as the core problem-solving tasks.
Finally, everything else, emailing and scheduling, resolving meeting topics, writing down new problem-solving approaches were done on foot, e.g., while watching my son at the park.</p>

<h2 id="closing-thoughts">Closing thoughts</h2>

<p>Even after trying all the different tricks, I still missed countless deadlines and took countless bullets of consequences.
When I felt like I hit my limits, the only thing I could do was changing my mentality.
I accepted my limit, lowered bars for myself, thought about the meaning of what I’m desperately chasing after in the grand skim of things, and try to enjoy the process more.</p>

<p>Time to time, I ask myself–was it all worth it?
As much as I try rewiring my brain to reply with “yes” to that question, I cannot bring back the times I missed with my family and I’ll always feel the guilt of not being around much (physically or mentally) when my son needed me the most.
Yet, I’m working on this blog post while watching my son at a playground.</p>

<p><br /></p>

<h3 id="acknowledgments">Acknowledgments</h3>

<p>I consciously and unconsciously picked up the habits of other parents who I closely worked with and had much more responsibility than me like my grad school advisor <a href="https://homes.cs.washington.edu/~mcakmak/">Maya Cakmak</a>, and my past team leader <a href="http://chfritz.github.io/">Christian Fritz</a>.</p>

<p>I decided to write this blog post when I discovered the existence of similar blog posts from authors I respect:</p>
<ul>
  <li><a href="https://medium.com/bits-and-behavior/how-i-sometimes-achieve-academic-work-life-balance-4bbfc1769820">“How I (sometimes) achieve academic work life balance”</a> by Amy Ko</li>
  <li><a href="https://raymondcheng.net/thoughts/time-management.html">“Time Management”</a> by Raymond Cheng</li>
  <li><a href="https://maxwellforbes.com/posts/appropriate-quality">“Appropriate Quality”</a> by Maxwell Forbes</li>
</ul>

<p>Last but not least, I have to disclose that I relied a lot on my wife who sacrificed her time for watching our son.
This note might change the perspective/legitimacy of all tricks I mentioned above.</p>

<p><br /></p>

<h4 id="footnotes">Footnotes</h4>

<p>[^1] This trick was somewhat inspired by <a href="https://www.productplan.com/glossary/gist-planning/">GIST Planning</a> and <a href="https://spark-public.s3.amazonaws.com/startup/lecture_slides/lecture5-market-wireframing-design.pdf">Startup Engineering</a>.
<br />[^2] <a href="https://www.google.com/search?q=Eisenhower+Matrix&amp;tbm=isch">The Eisenhower matrix</a> is a great task prioritization technique, however, in my experience, applying the technique in practice, specifically, classifying tasks clearly into one of 4 slots, wasn’t trivial and hence had a similar problem.
<br />[^3] Often, combined smaller tasks such as a task decomposition task combined with an execution task also required my full attention.
<br />[^4] I consider these tasks as combinatorial optimization problems and try to act like well-known algorithms such as branch and bound or iterative deepening when exploring paths to solutions.</p>]]></content><author><name></name></author><category term="#productivity" /><category term="#gradschool" /><category term="#dadlife" /><summary type="html"><![CDATA[I was a graduate student when my son was born. As much as I was euphoric about my son’s arrival, reality hit me hard. I was not ready. It was brutal but I pulled through graduate school and here is s selected list of tricks I’ve learned from having been a graduate student parent.]]></summary></entry><entry><title type="html">Job Searching for an Industry Position after Graduate School</title><link href="/2020/11/15/job.html" rel="alternate" type="text/html" title="Job Searching for an Industry Position after Graduate School" /><published>2020-11-15T08:00:00+00:00</published><updated>2020-11-15T08:00:00+00:00</updated><id>/2020/11/15/job</id><content type="html" xml:base="/2020/11/15/job.html"><![CDATA[<p>Earlier this year, I started to look for a job and one of my friends recommended <a href="https://bharathpbhat.github.io/2020/09/19/laid-off-now-what.html">this post</a> written by <a href="https://twitter.com/bharathpbhat">Bharath</a>, a former Uber ML engineer who also had to find a job earlier-er this year but in a much more stressful situation, i.e., within 60 days.
The blog post was amazing.
I basically followed the author’s process with some adjustments for myself, a human-robot interaction researcher who just finished grad school.
In this post, I’ll talk about my job search experience and my adopted process based on Bharath’s process.</p>

<h2 id="on-identifying-which-rolecompany-to-apply">On identifying which role/company to apply</h2>

<p>For a fresh-out-of-school, human-robot interaction researcher with some experience in software engineering work in the industry, the biggest challenge was people didn’t know what was I good at or what I wanted to do.
A part of it was due to the nature of the human-robot interaction or robotics research field that has a wide range of subfields.
Another part of it was me; I had the roles I wanted to take in mind but I was not sure whether I will be good at it so the companies will hire me for the role.
One of my mentors recommended <a href="https://medium.com/thrive-global/ikigai-the-japanese-secret-to-a-long-and-happy-life-might-just-help-you-live-a-more-fulfilling-9871d01992b7">ikigai</a> for career-related decision making and it helped me take the first pass on identifying which companies to apply.
After that, I decided to apply to a wide range of roles (e.g., Software Development Engineer, Applied Researcher, Product Engineer, etc.) across a wide range of companies (e.g., ~5 people startups to BigCos) to find the right role by interacting with companies.</p>

<p>I started by following “The Process” in the “Reach out to everyone” section in Bharath’s blog.
I made a list of companies, reached out to people related to a target company, iterated those two steps until I had a list of ~20 companies that would talk to me.
I reached out to over 50 people–seniors, juniors, friends, friends of the family, any who would talk to me about a role similar to the ones I identified earlier.
The reaching out process was laborious and sometimes humiliating but it was extremely important in retrospect as it turns out some very interesting roles that were not public were found this way.</p>

<p>Sometimes the “speaking with a hiring manager” step naturally happened and I’ve used Bharath’s notes for preparing myself for those meetings.
Whenever I get to talk to a hiring manager or an employee at the team I want to join, I tried to ask as many questions as possible about the role, e.g., a list of selected questions from related articles like <a href="https://www.indeed.com/career-advice/interviewing/questions-to-ask-a-company">this</a> and <a href="https://angel.co/blog/30-questions-to-ask-before-joining-a-startup">this</a> to really visualize what I would be doing in my 1st year and after.
I also asked some questions about the interview process and interview tips to tailor my preparation to a particular company, if needed.
For example, smaller companies’ interviews were less structured while big companies’ interviews’ were highly structured, e.g., for Amazon, see <a href="https://www.amazon.jobs/en/landing_pages/in-person-interview">this</a>.
After talking to a hiring manager, I was able to gauge 1. how interested the company was in hiring me and 2. what they were looking for (e.g., robotics application building, robotics user interface design, evaluating interactive robot systems, etc).
I was also able to feel out whether I have the experiences and skills they were looking for.
This was extremely useful for narrowing down the list because I was able to ask myself how much effort do I want to put in trying to convince a company why they need me or how quickly I can learn the missing skills.
After this step, the number of companies in my list reduced to ~10.
I followed up with a recruiter or hiring manager to start the first step of the interview process, which usually was a coding interview.</p>

<h2 id="preparing-for-coding-interviews">Preparing for coding interviews</h2>

<p>Coding interviews always scare me.
Following Bharath’s process, I started solving a couple of <a href="https://leetcode.com/">leetcode</a> problems every week, was <a href="https://youtu.be/GbyXxUDVeAo?t=105">being very selective with which leetcode problems to work on</a>, and <a href="https://medium.com/hackernoon/14-patterns-to-ace-any-coding-interview-question-c5bb3357f6ed#9cb9">studied patterns</a> and <a href="https://twitter.com/sunilc_/status/1304722881503395840">categories</a> of coding problems to identify my weakest patterns and categories.
Even after such preparation, I choked during the first couple of coding interviews but was much more comfortable in later coding interviews.
For my coding interviews, most companies used a shared coding platform like <a href="https://coderpad.io/">CoderPad</a> and others asked me to share my desktop screen to see how I code in my own environment; some, usually smaller companies, gave me “homework” or a tiny project to work on.
I liked live-coding interviews with a shared coding platform because it saved my time the most.</p>

<h2 id="preparing-for-system-design-interviews">Preparing for system design interviews</h2>

<p>Bharath said system design interviews were his favorite.
For me, system design interviews were the most difficult.
First, the existing system design interview guidelines (like <a href="https://github.com/donnemartin/system-design-primer">this</a> (free) and <a href="https://www.educative.io/courses/grokking-the-system-design-interview">this</a> (paid)) were not tailored to the robotics problems, and second, I’ve learned that system design interview experiences varied a lot across the companies.
I also had a hard time finding a friend or peer who would act as an interviewer to help me with the preparation.
I started by <a href="https://docs.google.com/document/d/14ePsRiubmrbnK3Pm2ETaA9PYNDun24l8XgGR44ILyC4/edit?usp=sharing">creating robotics system design questions</a> based on existing <a href="https://github.com/donnemartin/system-design-primer#system-design-interview-questions-with-solutions">example system design questions with solutions</a>.
Here are other questions I’ve considered:</p>

<ul>
  <li>Design an object detector for a mobile manipulator robot for pick-up tasks</li>
  <li>Design a collaborative robot manipulator for an assembly task</li>
  <li>Design a teleoperation interface for a mobile robot</li>
</ul>

<p>However, based on my interview experiences, some system design questions I’ve asked required having good intuitions on robotics (or robotics perception or motion planning) algorithms to be able to discuss the trade-offs of using different approaches and practical implications for building robotics systems.
Or an ability to map the questions that seem not-so-related to a robotics problem to a robotics system design problem and discuss the approaches and trade-offs or related issues the interviewers are looking for.
Based on post-interview feedback, the interviewers seemed to look for the interviewee’s ability to clearly <em>communicate</em> to gather requirements, identify a problem, propose multiple approaches, discuss trade-offs, and making calculated decisions–ideally while demonstrating experiences in related, industry-standard tools and frameworks.</p>

<h2 id="preparing-for-core-concept-interviews">Preparing for core concept interviews</h2>

<p>For me, a very few interviews involved asking about core (robotics) concepts; probably because I only made/pursued a very few research positions.
Here I followed Bharath’s process and created a <a href="https://docs.google.com/document/d/1q3_Vu2BdXFafyGuRM4I1HHtWo-Gd041rvC04FytmG9U/edit?usp=sharing">basic ML &amp; robotics concepts list for myself</a>.
For each algorithm, I asked the following four questions:</p>

<ul>
  <li>What is it?</li>
  <li>How does it work? (time/space complexity?)</li>
  <li>When do you use it?</li>
  <li>What are the limitations? Practical considerations?</li>
  <li>Anything else? (personal experiences and findings, etc.)</li>
</ul>

<h2 id="preparing-for-behavioral-interviews">Preparing for behavioral interviews</h2>

<p>Bharath’s notes helped prepare behavioral interviews.
One strategy I like to emphasize is tailoring stories for an interviewer or company.
Tell stories about technical success stories to engineers, research success stories to researchers, leadership success stories to managers.</p>

<h2 id="closing-notes">Closing notes</h2>

<p>I want to re-emphasize the importance of sourcing many interview opportunities.
My peers recommended doing this as one may not know whether a role is interesting until talking to people in the team (i.e., reading job descriptions are not enough).
Another important factor in hindsight is timing.
I considered job searching in May and June and I could not get any interviews.
I got lucky and was able to delay the search for about 2 months and there were a day and night difference.
Timing is something one may not have control over, but if you do, talk to many people (and use other resources) to see how many opportunities are/will be there in your job search time frame.</p>]]></content><author><name></name></author><category term="#gradschool" /><category term="#jobsearch" /><summary type="html"><![CDATA[Earlier this year, I started to look for a job and one of my friends recommended this post written by Bharath, a former Uber ML engineer who also had to find a job earlier-er this year but in a much more stressful situation, i.e., within 60 days. The blog post was amazing. I basically followed the author’s process with some adjustments for myself, a human-robot interaction researcher who just finished grad school. In this post, I’ll talk about my job search experience and my adopted process based on Bharath’s process.]]></summary></entry><entry><title type="html">Getting started with robotics: Just do it!</title><link href="/2019/12/15/getting.html" rel="alternate" type="text/html" title="Getting started with robotics: Just do it!" /><published>2019-12-15T08:00:00+00:00</published><updated>2019-12-15T08:00:00+00:00</updated><id>/2019/12/15/getting</id><content type="html" xml:base="/2019/12/15/getting.html"><![CDATA[<p>Getting started with robotics is confusing.
Robotics is an interdisciplinary field and people think of many different things when they are trying to learn about it.
For example, google searching “getting started with robotics” gives me the following top three results:</p>

<ul>
  <li><a href="https://www.youtube.com/watch?v=uw-4K9joFL8">How To Start With Robotics? - YouTube</a></li>
  <li><a href="http://robotsforroboticists.com/getting-started-kids-adults/">Robotics for Kids (and Adults) – Getting Started and How to Progress</a></li>
  <li><a href="https://robots.ieee.org/learn/getting-started/">Getting Started in Robotics - ROBOTS: Your Guide to the World of Robotics</a></li>
</ul>

<p>They talk about learning skills related to the fields of mechanical engineering, electrical engineering, and computer science.
At first, it just felt overwhelming.
Reading each of them slowly again, they were great tutorials especially because they all shared one great message–“learn by doing projects” (<a href="https://www.amazon.com/Robotics-Project-Based-Approach-Lakshmi-Prayaga-ebook/dp/B00PG922M4">there was even a book named with a similar spirit</a>).</p>

<p>I 100% agree with the message, I think people should learn robotics by doing projects.
In fact, I recently shared <a href="https://github.com/mjyc/awesome-robotics-projects">my curated list of opensource (and other) robotics projects</a> for those who are interested in building robots.
Because I’m a programmer by training, one additional suggestion I like to add is “start by working with a simulator”.
Working with hardware is fun but it can be extremely time-consuming so by working with a simulator first you can feel out the robot and identify potential problems early.
Projects like <a href="https://mushr.io/">MuSHR</a> and <a href="https://hackaday.io/project/164992-bobble-bot">bobble-bot</a> are great because they provide robot simulators as well as detailed instructions for building robots.
<a href="https://atsushisakai.github.io/PythonRobotics/">PythonRobotics</a> is another great entry point for learning about robotics algorithms.
The repository contains provide tiny, simple environments for testing the algorithms which are great for learning purposes.
Here is a list of <a href="https://www.ros.org/">ROS</a>-based simulators that I’ve curated in <a href="https://rds.theconstructsim.com/r/mchung/">ROS Development studio</a>, <a href="https://www.theconstructsim.com/rds-ros-development-studio/">a cloud service</a> that allows you to work on ROS projects in a browser.
In a similar spirit, I encourage using a single board computer such as <a href="https://www.raspberrypi.org/">Raspberry Pi</a> or <a href="https://developer.nvidia.com/embedded/learn/tutorials">NVIDIA Jetson products</a> instead of using a microcontroller like <a href="https://www.arduino.cc/">Arduino</a>.
Programming a microcontroller can be fun and it can allow you to develop a solution that is highly tailored to your use case, but for learning purposes, it can become a rabbit hole that prevents you from completing the project you started.
However, if your goal is learning mechanical or electrical engineering my advice (rather opinions) is not for you.</p>

<p>Finally, I believe getting involved with robotics communities is effective for learning.
The below list could be good entry points for learning about software-focused robotics</p>

<ul>
  <li><a href="https://github.com/topics/robotics">github repos with #robotics tag</a></li>
  <li><a href="https://discourse.ros.org/">ROS discourse</a></li>
  <li><a href="https://foxglove.dev/blog">Foxglove blog</a></li>
  <li><a href="https://picknik.ai/blog/">PICKNIK blog</a></li>
  <li><a href="https://developer.nvidia.com/blog/tag/isaac-sim/">Isaac Sim Technical Blog</a></li>
  <li><a href="https://www.duckietown.org/research/ai-driving-olympics">The AI Driving Olympics (AI-DO)</a></li>
  <li><a href="https://www.balena.io/blog">Balena blog</a> - they provide less robotics and more IoT-centric contents</li>
  <li><a href="https://getcruise.com/news">Cruise news</a></li>
  <li><a href="https://github.com/mjyc/awesome-robotics-system-design">Awesome Robotics System Design</a> - where I keep interesting software-focused robotics stuff</li>
  <li><a href="https://github.com/mjyc/awesome-robotics-problems-design">Awesome Robotics Problems</a> - where I keep interesting robotics problems, datasets, and algorithm implementations</li>
</ul>

<p>the list below for learning about electronics-focused robotics</p>

<ul>
  <li><a href="https://www.sparkfun.com/news">sparkfun news</a></li>
  <li><a href="https://blog.adafruit.com/">adafruit blog posts</a></li>
</ul>

<p>and the list below for learning about hardware-focused robotics</p>

<ul>
  <li><a href="https://www.instructables.com/">instructables</a></li>
  <li><a href="https://hackaday.com/">hackaday</a></li>
  <li><a href="https://www.hackster.io/">hackster.io</a></li>
  <li><a href="https://www.onshape.com/en/blog/">onshape blog</a> - <a href="https://hackaday.com/2021/02/28/onshape-to-robot-models-made-easier/">roboticsts love it</a></li>
</ul>

<p>This may be a bit off topic, but since people relate “robotics” with AI/ML computer science research, it might be fun to skim robotics-related papers in open paper review and curated paper list websites:</p>

<ul>
  <li><a href="https://arxiv.org/">https://arxiv.org/</a></li>
  <li><a href="https://openreview.net/">https://openreview.net/</a></li>
  <li><a href="https://paperswithcode.com/">https://paperswithcode.com/</a> - one note: not all researchers are great coders/documenters.</li>
  <li><a href="http://bohg.cs.stanford.edu/list/">http://bohg.cs.stanford.edu/list/</a></li>
</ul>

<p>Talking about skimming, it might be inspiring to skim the class materials from <a href="https://courses.cs.washington.edu/courses/cse478/20wi/">CSE 478: Autonomous Robotics</a>.
Unlike many other class materials, their class slides provide application examples of introduced concepts with an open-source autonomous mobile robot platform <a href="https://mushr.io/">MUSHR</a>.</p>

<p><strong>WARNING</strong> Reading papers and learning class materials can become yet another rabbit hole.
There are endless interesting papers (on surface) or concepts (from class slides) and they can distract you from finishing your project.
What happens is that because you feel achievement/growth and you get temped to keep learning.
Being able to focus on the track and learn only necessary skills (and taking the project to the finish line–and defining the finish line) is a huge challenge/probably the most important skill to learn.</p>

<p>With that said, go explore project ideas, check out robotics communities and start your project!
I believe now is the time to learn about robotics and I hope this blurb can be helpful to aspiring roboticists.</p>]]></content><author><name></name></author><category term="#beginners" /><summary type="html"><![CDATA[Getting started with robotics is confusing. Robotics is an interdisciplinary field and people think of many different things when they are trying to learn about it. For example, google searching “getting started with robotics” gives me the following top three results:]]></summary></entry><entry><title type="html">Consumer Robots are Dead, Long Live DIY Robots!</title><link href="/2019/06/23/consumer.html" rel="alternate" type="text/html" title="Consumer Robots are Dead, Long Live DIY Robots!" /><published>2019-06-23T08:00:00+00:00</published><updated>2019-06-23T08:00:00+00:00</updated><id>/2019/06/23/consumer</id><content type="html" xml:base="/2019/06/23/consumer.html"><![CDATA[<p>In the span of a year, we have witnessed the death of major consumer robot companies.
What went wrong?
Guy Hoffman, a robotics expert, <a href="https://spectrum.ieee.org/anki-jibo-and-kuri-what-we-can-learn-from-social-robotics-failures">provides a comprehensive summary</a> of possible reasons for the demise of the companies.
To me, the problem was good old over-promise and under-deliver.
I mean, just look at <a href="https://youtu.be/H0h20jRA5M0">this commercial</a> and look at <a href="https://youtu.be/xmntMiJ5zKs">a real robot</a>.
But I also think that there is another problem that is rooted in our culture; when you hear the word “robot”, what comes into your mind?
I think about C-3PO and R2-D2 from Star Wars, The Terminator, WALL-E, Sonny from I, Robot, and Marvin from The Hitchhiker’s Guide to the Galaxy, just to name a few.
Most of these robots are physically more capable than humans  and even emotionally as capable as humans[^1].
After seeing such robots over and over, we expect a robot to be a super awesome friend/servant (or killing machine), and see a commercial like <a href="https://youtu.be/H0h20jRA5M0">this</a> and we get <a href="https://youtu.be/xmntMiJ5zKs">this</a>[^2].
But I digress.</p>

<p>Of course not all consumer robots are dead.
We still have iRobot’s Roomba and Amazon’s Echo/Alexa–if you consider a voice-agent compatible smart speaker a robot.
But for some reason, they don’t feel like a robot.
I nearly gave up on trying to define what the “robot” is and advocating for <a href="https://twitter.com/mjycio/status/1300898349529182208">not calling anything a “robot”</a>, partially to stop that cultural image of the “robot” mentioned earlier.
But that didn’t work, e.g., I couldn’t stop using the word “robot”.
So I made peace with considering a physical device (e.g., a mobile robot or robot manipulator) that is capable of complex sensing (e.g., computer vision), complex control (e.g., motion planning), and adaptation (e.g., machine learning) a “robot”.
For example, I wouldn’t call industrial manipulators that pick things up from and place things in known locations but I would call a mobile rover that recognizes street signs and avoids pedestrians crossing a road a robot.</p>

<p>Okay, so where do we go from here?
Will we have a “robot” that we can use at home? what will it look like? what will it do?
It’s likely that a big tech company will build something, but I say the time is now for rolling our sleeves up and building home robots ourselves!
It might be difficult but I think it is possible.
First, we need hardware.
We can buy a kit like a TurtleBot or get an open hardware design from one of the Hackaday blog posts or design one from scratch on onshape, if you can.
Second, we’ll need to add electronics.
We can buy a Raspberry Pi or a super fancy GPU or TPU board from <a href="https://developer.nvidia.com/embedded/jetson-nano">NVidia</a> or <a href="https://coral.ai">Google</a>.
I can’t work with microcontrollers but don’t let me stop you.
Finally, software.
There are opensource softwares like ROS, Nvidia Isaac, Apex Autoware, etc.</p>

<p>What should we build?[^3]
That’s a great question.
My approach is reviewing my hobbies and asking how can I use robotics capabilities to make them more interesting.
This usually boils down to adding capabilities like mobility, manipulability, or computer vision to existing things. I tried to build interactive or mobile decorations like interactive lights (e.g., changing ceiling light colors based on locations of detected people) and mobile pots (e.g., moving to different locations at different times).
Another approach is starting from the problem, e.g., by asking what problem can I solve with robotics capabilities?
This approach usually gets blocked by the current limitations of the robotics tech, but some people do come with clear solutions (e.g., limiting the scope of the problem, involving humans, etc.) to make progress.</p>

<p>Honestly though, it isn’t easy to build homemade robots and making them actually useful is near-impossible for a hobbyist.
Although robotics technology is advancing quickly, there are limitations that makes them not as reliable for real world use cases and hardware devices (e.g., high-quality motors) are still pretty expensive.
So why did I suggest to build your own robot?
I wanted the robot lovers to acknowledge the fact that we are building home robots because we just love doing so[^4] and you shouldn’t be discouraged by recent fallings of the robot companies.
Long live DIY robots!</p>

<p><br /></p>

<h4 id="updates">Updates</h4>

<ul>
  <li><em>2023/05/06</em> Recently, I heard that <a href="https://techcrunch.com/2023/05/01/neato-robotics-is-being-shut-down-after-18-years/">Neato Robotics was shutting down</a>. Unlike other shutdown news (e.g., <a href="https://www.theverge.com/2023/2/24/23613214/everyday-robots-google-alphabet-shut-down">Everyday Robots shutdown</a>), this news was particularly shocking because I thought robot vacuums sell. Couple other observations since the first draft of this post: (1) new consumer/home robots were announced, e.g., <a href="https://labradorsystems.com/">Labrador</a>, <a href="https://www.aboutamazon.com/news/devices/meet-astro-a-home-robot-unlike-any-other">Amazon Astro</a>, and <a href="https://www.tiktok.com/@arina.bloom/video/7221590486514027818">Matician Matic</a>, (2) there are many open-source or DIY autonomous RC car projects (e.g., <a href="https://f1tenth.org/">F1TENTH</a>, <a href="https://www.duckietown.org/">Duckiebot</a>, <a href="https://www.diyrobocars.com/">DIY Robocars</a>, <a href="https://aws.amazon.com/deepracer/">DeepRacer</a>, <a href="https://mushr.io/">MuSHR</a>, <a href="https://racecar.mit.edu/">MIT Racecar</a>) appeared, and (3) humanoid robot companies raised $$$, e.g., <a href="https://agilityrobotics.com/news/2022/future-robotics">Agility Robotics</a> and <a href="https://www.figure.ai/">Figure AI</a>.</li>
</ul>

<h4 id="footnotes">Footnotes</h4>

<p>[^1] I love fictions/movies like <a href="https://en.wikipedia.org/wiki/Pluto_(manga)">Pluto</a> or <a href="https://en.wikipedia.org/wiki/Her_(film)">Her</a> that questions the meaning of “human” when robots can be as capable as humans.
<br />[^2] I keep picking on Jibo but I really wanted it to succeed and at this point, I think learning from its mistake is important for roboticists of tomorrow.
<br />[^3] Also checkout <a href="https://generalrobots.substack.com/p/how-to-pick-a-problem">How to Pick a Problem</a>
<br />[^4] Checkout <a href="https://www.robinsloan.com/notes/home-cooked-app/">AN APP CAN BE A HOME-COOKED MEAL</a> by Robin Sloan</p>]]></content><author><name></name></author><category term="#culture" /><summary type="html"><![CDATA[In the span of a year, we have witnessed the death of major consumer robot companies. What went wrong? Guy Hoffman, a robotics expert, provides a comprehensive summary of possible reasons for the demise of the companies. To me, the problem was good old over-promise and under-deliver. I mean, just look at this commercial and look at a real robot. But I also think that there is another problem that is rooted in our culture; when you hear the word “robot”, what comes into your mind? I think about C-3PO and R2-D2 from Star Wars, The Terminator, WALL-E, Sonny from I, Robot, and Marvin from The Hitchhiker’s Guide to the Galaxy, just to name a few. Most of these robots are physically more capable than humans and even emotionally as capable as humans[^1]. After seeing such robots over and over, we expect a robot to be a super awesome friend/servant (or killing machine), and see a commercial like this and we get this[^2]. But I digress.]]></summary></entry><entry><title type="html">Collaborating with Undergraduate Students as a Graduate Student in Research</title><link href="/2019/06/04/collaborating.html" rel="alternate" type="text/html" title="Collaborating with Undergraduate Students as a Graduate Student in Research" /><published>2019-06-04T08:00:00+00:00</published><updated>2019-06-04T08:00:00+00:00</updated><id>/2019/06/04/collaborating</id><content type="html" xml:base="/2019/06/04/collaborating.html"><![CDATA[<p>Although there are great blog posts on this topic of “how to work with undergraduate students” from veterans in the field [<a href="https://homes.cs.washington.edu/~mernst/advice/undergrad-research.html">1</a>,<a href="https://www.cs.cornell.edu/~asampson/blog/undergrads.html">2</a>], I have my own take on this topic so here I wrote down the process I came up with from working with truly amazing undergraduate students I met over my graduate school years.</p>

<h4 id="step-1-identify-your-goal">Step 1: Identify your goal.</h4>

<p>First, you should clearly understand what do <em>you</em> want to get out by working with undergraduate students.
Do you need some help on finishing up a small portion of your research project?
Are you excited about your research topic and do you want to have someone else take a look at unexplored ideas?
Do you want to accelerate the growth of your field by outreaching to undergraduate students?
Whatever your goal is, you want to be clear about it so you can prepare an appropriate interview and collaboration strategies.</p>

<p>Note that goals can be updated over time.
Personally, I like to set my initial goal as getting help on a concrete and small subset of my project, then update the goal to help the student to become a researcher–if and only if such goal seems mutually beneficial.</p>

<h4 id="step-15-find-a-student">Step 1.5: Find a student.</h4>

<p>The easiest approach is doing nothing.
In a University setting, motivated undergraduate students will email your advisor or you to get involved in a research project and you should be wanting to work with those motivated students.
The obvious pro of this approach is that you don’t need to do anything.
The downside is that you cannot control the timing and sometimes even the project topic you’ll be collaborating on.
For example, your advisor may email you to work with a student who comes with their own funding and research topic over the summer.</p>

<p>The second easiest approach is attending “undergraduate research fair” type events hosted by your department or University.
This approach gives you more control in timing and the collaboration project topic but requires you to put some effort on preparing materials for the fair and more importantly, I found meeting a suitable or talented student hit-or-miss.</p>

<p>The final approach is asking around.
If you can elaborate kind of students you are looking for clearly, your peers or advisors can be helpful in finding a suitable student for you.
Compare to the “attending research fairs” approach, this approach can help you find “the student” you are looking but with a low probability of actually finding one.</p>

<p>I tend to take both the first and third approaches.
The downside of the two approaches are having no control in the timing but if you have multiple projects going at different stages all the time, you always have a project to collaborate with, so timing becomes less of an issue than finding a talented, passion-sharing student.</p>

<p>Finding a student requires you to have an interview strategy.
For this topic, I highly recommend checking out Michael Ernst’s <a href="https://homes.cs.washington.edu/~mernst/advice/interviewing-undergraduates.html">“Tips for interviewing undergraduates for research”</a>.</p>

<h4 id="step-2-identify-your-students-goal">Step 2: Identify your student’s goal.</h4>

<p>Understanding your student’s goal is as important as understanding your own goal for a healthy collaboration relationship.</p>

<p>From my experience, the most common motivation of the undergraduate students I’ve work with was getting exposed to robotics research.
For the older students who are closer to graduate, they were also motivated by expanding their skill sets to make themselves more attractive to the companies they want to apply to.
There were also some students who wanted to publish an academic paper before they apply to a graduate school to strengthen their application.</p>

<p>I found that it is difficult to identify undergraduate students’ internal motivations from interviews alone.
Most students do not fully understand what their own goals are or interests since they are not fully aware of what they really want or simply because they had not had much experience with robotics research.
Therefore I like to plan the first collaboration project with a student as a “getting to know each other” project.
However, if you don’t have time, e.g., working on a short term project, you will need to rely on your intuition on identifying students’ motivations.</p>

<h4 id="step-3-start-collaborating">Step 3: Start collaborating.</h4>

<p>Your collaboration strategies should be dependent on your and your student’s goals and the agreed collaboration duration.</p>

<p>In general, I like to take a goal-driven collaboration strategy.
For example, I set the goals for students and myself for a certain duration and work towards those goals while helping each other.
This approach particularly worked well for me because by not giving them step-by-step instructions, the students show their original approaches for achieving the assigned project goals as well as their personality.
The approach also allows me to see whether they can learn from demonstrations.
Because we are collaborating, they see what I do to solve complex problems (or the problems that are unfamiliar to them) and give them chance to immediate my approach–if it makes sense.
I focus on demonstrating high-level problem-solving strategies like clearly identifying underspecified goals, breaking down complex goals, and managing one’s time and attention span.
Interestingly, over the years of trying out the goal-driven collaboration, I’ve learned a lot from the students as well.
Many students I met had less experience with the field of robotics, however, some of them were just truly gifted human beings and I could learn a lot from their original way of handling the small research projects give to them.</p>

<p>Sometimes, students are not ready to collaborate at all but you still want to or need to work with them.
In such a case, I give them a small-scoped, independent project and observe what they do.
I ask the following questions to myself to decide whether I want to work with the student for a longer-term:</p>

<ul>
  <li>Can they communicate?</li>
  <li>Can they learn?</li>
  <li>Can they apply the technical skill sets they claimed to have?</li>
  <li>Do they find research interesting?</li>
</ul>

<p>How do you know your student is making progress?
Look at what they do.
They will demonstrate that they have learned the required skill by applying them properly.
They will demonstrate that they are excited about the project by spending time on it over the weekend and by proactive contacting you to show what they have done.</p>

<p>Regardless of taking the goal-driven collaboration approach or the giving independent project approach and because my goal for collaborating with students is getting them involved into the world of research, i.e., finding future peers, I like to give them a snapshot of the graduate student lifestyle by encouraging to participate in lab meetings, research seminars and discussions with peers.
I also encourage them to present the potential impact of the collaboration project in their own words and encourage them to present their work as much as possible–at their class, lab meetings, or even at local meetups.
If they don’t have much time to do research work, I encourage them to use the research project for their graduation requirements, e.g., a project for a project-oriented senior class or senior thesis.</p>

<h4 id="step-4-say-goodbye">Step 4: Say goodbye.</h4>

<p>Most times, students leave the project in less than six months; some graduate, move on to another project, or simply stop working on it.
What about when things are going well?
How do you know when to stop collaborating?
Eventually, when to part away with your student will become obvious.
They will naturally work more independently and even start identifying their own agendas and work towards them, e.g., by working with your advisor directly.
That would be time to stop considering them as an “undergraduate” researcher and treat them like a peer researcher.</p>]]></content><author><name></name></author><category term="#gradschool" /><summary type="html"><![CDATA[Although there are great blog posts on this topic of “how to work with undergraduate students” from veterans in the field [1,2], I have my own take on this topic so here I wrote down the process I came up with from working with truly amazing undergraduate students I met over my graduate school years.]]></summary></entry></feed>